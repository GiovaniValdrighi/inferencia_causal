{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "scm_counterfactual.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOnv5DIFIS8oo/uWeilnBF2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GiovaniValdrighi/inferencia_causal/blob/master/vae/scm_counterfactual.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XK1wfSf5rDRM",
        "colab_type": "code",
        "outputId": "6327948b-ef1f-4d95-cd10-47d73b781e9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apUQA1WZrWNH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9rBbgcKrWma",
        "colab_type": "code",
        "outputId": "68150acb-3c49-4cd9-a881-7e4014574f00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        }
      },
      "source": [
        "!pip3 install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.0-{platform}-linux_x86_64.whl\n",
        "!pip3 install torchvision\n",
        "!pip3 install pyro-ppl"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: torch-0.4.0-{platform}-linux_x86_64.whl is not a valid wheel filename.\u001b[0m\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.17.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (6.2.2)\n",
            "Requirement already satisfied: torch==1.4.0 in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.12.0)\n",
            "Requirement already satisfied: pyro-ppl in /usr/local/lib/python3.6/dist-packages (1.2.1)\n",
            "Requirement already satisfied: pyro-api>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (0.1.1)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (1.17.5)\n",
            "Requirement already satisfied: tqdm>=4.36 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (4.42.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (3.1.0)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (1.4.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bS6uj2clreyt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from vae_build import VAE, ind_from_att, label_from_dummy\n",
        "import pyro\n",
        "import matplotlib.pyplot as plt\n",
        "from pyro.distributions import OneHotCategorical, RelaxedOneHotCategorical, Normal, Uniform, constraints\n",
        "from torch import tensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2zR_RmLrpmr",
        "colab_type": "code",
        "outputId": "dc35861c-0c5d-4a48-ce16-184dda0d0e38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "dataset_zip = np.load('/content/gdrive/My Drive/autoencoder/dsprites_ndarray_co1sh3sc6or40x32y32_64x64.npz', allow_pickle = True, encoding = 'bytes')\n",
        "print('Keys in the dataset:', dataset_zip.files)\n",
        "imgs = dataset_zip['imgs']\n",
        "latents_values = dataset_zip['latents_values']\n",
        "latents_classes = dataset_zip['latents_classes']\n",
        "latents_sizes = dataset_zip['metadata'][()][b'latents_sizes']\n",
        "latents_names = dataset_zip['metadata'][()][b'latents_names']"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Keys in the dataset: ['metadata', 'imgs', 'latents_classes', 'latents_values']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZR3o4vQrq6p",
        "colab_type": "code",
        "outputId": "d4c2bd2a-2590-4f7c-c062-d81129c429ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pyro.enable_validation(True)\n",
        "pyro.clear_param_store()\n",
        "#the training routine\n",
        "use_CUDA = False\n",
        "vae = VAE(latents_sizes, latents_names, use_CUDA = use_CUDA)\n",
        "vae.load_state_dict(torch.load('/content/gdrive/My Drive/trained_movel_epoch_24.save'))"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skuKdZ2uvBd-",
        "colab_type": "text"
      },
      "source": [
        "def condition(cond_values, n_steps = 1000):\n",
        "      #calculating noise posterior in the observation model\n",
        "      condModel = pyro.condition(model, data = cond_values)\n",
        "      pyro.clear_param_store()\n",
        "\n",
        "      #setting the SVI attributes\n",
        "      adam_params = {'lr': 0.001}\n",
        "      optmizer = pyro.optim.Adam(adam_params)\n",
        "      elbo = pyro.infer.Trace_ELBO()\n",
        "      svi = pyro.infer.SVI(condModel, guide, optmizer, elbo)\n",
        "\n",
        "      #training loop\n",
        "      losses = []\n",
        "      mu_samples = {key: [] for key in self.init_noise.keys()}\n",
        "      sigma_samples = {key: [] for key in self.init_noise.keys()}\n",
        "\n",
        "      for i in range(n_steps):\n",
        "        losses.append(svi.step(self.init_noise))\n",
        "        #saving samples\n",
        "        for key in self.init_noise.keys():\n",
        "          mu_samples[key].append(pyro.param('mu_'+key))\n",
        "          sigma_samples[key].append(pyro.param('sigma_'+key))\n",
        "\n",
        "      #calculating means\n",
        "      mean_samples = {}\n",
        "      mean_samples['mu'] = {key: [] for key in self.init_noise.keys()}\n",
        "      mean_samples['sigma'] = {key: [] for key in self.init_noise.keys()}\n",
        "      for key in self.init_noise.keys():\n",
        "        mean_samples['mu'][key] = torch.mean(torch.stack(mu_samples[key]), dim = 0)\n",
        "        mean_samples['sigma'][key] = torch.mean(torch.stack(sigma_samples[key]), dim = 0)\n",
        "\n",
        "      print(mean_samples)\n",
        "      updated_noise = {}\n",
        "      for key in self.init_noise.keys():\n",
        "        updated_noise[key] = Normal(mean_samples['mu'][key], mean_samples['sigma'][key])\n",
        "\n",
        "      img_samples = []\n",
        "      for _ in range(1000):\n",
        "        _, _, s = model(updated_noise)\n",
        "        img_samples.append(s)\n",
        "      img_samples = torch.mean(torch.stack(img_samples), dim = 0)\n",
        "      print(losses)\n",
        "      plt.figure()\n",
        "      plt.plot(losses)\n",
        "      plt.figure()\n",
        "      plt.imshow(img_samples.detach().numpy().reshape(64, 64))\n",
        "      plt.show()\n",
        "      \n",
        "\n",
        "    def counterfactual(var_name, obs_var, counter_var, n_steps = 1000):\n",
        "      #calculating noise posterior in the observation model\n",
        "      obsModel = pyro.condition(model, data = {var_name : obs_var})\n",
        "      pyro.clear_param_store()\n",
        "\n",
        "      #setting the SVI attributes\n",
        "      adam_params = {'lr': 0.001}\n",
        "      optmizer = pyro.optim.Adam(adam_params)\n",
        "      elbo = pyro.infer.Trace_ELBO()\n",
        "      svi = pyro.infer.SVI(obsModel, guide, optmizer, elbo)\n",
        "\n",
        "      #training loop\n",
        "      losses = []\n",
        "      noise_samples = {latent:[] for latent in self.latents_names}\n",
        "      noise_samples['latent'] = {'mu':[], 'sigma': []}\n",
        "      noise_samples['img'] = {'low':[], 'high': []}\n",
        "\n",
        "      for i in range(n_steps):\n",
        "        losses.append(svi.step(self.init_noise))\n",
        "        #print(losses[-1])\n",
        "        #saving samples\n",
        "        for latent in self.latents_names:\n",
        "          noise_samples[latent].append(pyro.param('prob_'+latent))\n",
        "        noise_samples['latent']['mu'].append(pyro.param('mu_latent'))\n",
        "        noise_samples['latent']['sigma'].append(pyro.param('sigma_latent'))\n",
        "        noise_samples['img']['low'].append(pyro.param('low_img'))\n",
        "        noise_samples['img']['high'].append(pyro.param('high_img'))\n",
        "\n",
        "      #calculating means\n",
        "      mean_values = {latent:[] for latent in self.latents_names}\n",
        "      mean_values['latent'] = {'mu':[], 'sigma': []}\n",
        "      mean_values['img'] = {'low':[], 'high': []}\n",
        "      for latent in self.latents_names:\n",
        "        mean_values[latent] = torch.mean(torch.stack(noise_samples[latent]), dim = 0)\n",
        "      mean_values['latent']['mu'] = torch.mean(torch.stack(noise_samples['latent']['mu']), dim = 0)\n",
        "      mean_values['latent']['sigma'] = torch.mean(torch.stack(noise_samples['latent']['sigma']), dim = 0)\n",
        "      mean_values['img']['low'] = torch.mean(torch.stack(noise_samples['img']['low']), dim = 0)\n",
        "      mean_values['img']['high'] = torch.mean(torch.stack(noise_samples['img']['high']), dim = 0)\n",
        "\n",
        "      updated_noise = {}\n",
        "      for latent in self.latents_names:\n",
        "        updated_noise[latent] = OneHotCategorical(mean_values[latent])\n",
        "      updated_noise['latent'] = Normal(mean_values['latent']['mu'], mean_values['latent']['sigma'])\n",
        "      updated_noise['img'] = Normal(mean_values['img']['low'], mean_values['img']['high'])\n",
        "\n",
        "      #counterfactual query with updated noise\n",
        "      intModel = pyro.do(model, data = {var_name : counter_var})\n",
        "      cf_posterior = pyro.infer.Importance(intModel, guide, n_steps).run(updated_noise)\n",
        "      cf_marginal = pyro.infer.EmpiricalMarginal(cf_posterior, sites = 'img')\n",
        "      plt.imshow(cf_marginal.mean.detach().numpy().reshape(64, 64), cmap = 'Greys')\n",
        "      plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5nWEW9jsDnP",
        "colab_type": "code",
        "outputId": "d34acd7f-17c7-46bb-fc4d-0845243df60d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "class SCM():\n",
        "  def __init__(self, vae):\n",
        "    self.vae = vae\n",
        "    self.latents_names = ['color', 'shape', 'scale', 'orien', 'posX', 'posY']\n",
        "    self.latents_sizes = {'color' : 1, 'shape': 3, 'scale':6, 'orien': 40, 'posX': 32, 'posY':32}\n",
        "    dist = OneHotCategorical\n",
        "    self.init_noise = {'color': dist(tensor([1.])),\n",
        "                       'shape': dist(tensor([0.4, 0.4, 0.2])),\n",
        "                       'scale': dist(tensor([1/6]).repeat(6)),\n",
        "                       'orien': dist(tensor([1/40]).repeat(40)),\n",
        "                       'posX': dist(tensor([1/32]).repeat(32)),\n",
        "                       'posY': dist(tensor([1/32]).repeat(32)),\n",
        "                       'latent': Normal(torch.zeros(200), torch.ones(200)),\n",
        "                       'img': Uniform(torch.zeros(4096), torch.ones(4096))}\n",
        "    \n",
        "    #functions for the model\n",
        "    def f_gumbel(N):\n",
        "      return N\n",
        "    \n",
        "    def f_posX(N, scale):\n",
        "      if 31 <= int(scale.max(0)[1] + N.max(0)[1]):\n",
        "        return torch.nn.functional.one_hot(torch.tensor([31]), 32).to(torch.float32).reshape([32])\n",
        "      else:\n",
        "        return torch.nn.functional.one_hot(scale.max(0)[1] + N.max(0)[1], 32).to(torch.float32).reshape([32])\n",
        "\n",
        "    def f_latent(N_latent, color, shape, scale, orien, posX, posY):\n",
        "      ind = ind_from_att(color.max(0)[1], shape.max(0)[1], scale.max(0)[1], orien.max(0)[1], posX.max(0)[1], posY.max(0)[1])\n",
        "      label = torch.round(torch.cat([color, shape, scale, orien, posX, posY], -1))\n",
        "      mu, sigma = vae.encoder.forward(torch.tensor(imgs[ind]).reshape(4096).to(torch.float32), label)\n",
        "      return N_latent * sigma + mu\n",
        "\n",
        "    def f_image(N_img, latent, color, shape, scale, orien, posX, posY):\n",
        "      label = torch.round(torch.cat([color, shape, scale, orien, posX, posY], -1))\n",
        "      img_decode = vae.decoder.forward(latent, label)\n",
        "      return (N_img > img_decode).to(torch.float)\n",
        "\n",
        " \n",
        "    def model(noise = self.init_noise):\n",
        "\n",
        "      #Noise variables\n",
        "      N_color = pyro.sample('N_color', noise['color'])\n",
        "      N_shape = pyro.sample('N_shape', noise['shape'])\n",
        "      N_scale = pyro.sample('N_scale', noise['scale'])\n",
        "      N_orien = pyro.sample('N_orien', noise['orien'])\n",
        "      N_posX = pyro.sample('N_posX', noise['posX'])\n",
        "      N_posY = pyro.sample('N_posY', noise['posY'])\n",
        "      N_latent = pyro.sample('N_latent', noise['latent'].to_event(1))\n",
        "      N_img = pyro.sample('N_img', noise['img'].to_event(1))\n",
        "\n",
        "      #variables\n",
        "      color = pyro.sample('color', Normal(f_gumbel(N_color), torch.tensor([0.01])).to_event(1))\n",
        "      shape = pyro.sample('shape', Normal(f_gumbel(N_shape), torch.tensor([0.01])).to_event(1))\n",
        "      scale = pyro.sample('scale', Normal(f_gumbel(N_scale), torch.tensor([0.01])).to_event(1))\n",
        "      orien = pyro.sample('orien', Normal(f_gumbel(N_orien), torch.tensor([0.01])).to_event(1))\n",
        "      posX = pyro.sample('posX', Normal(f_posX(N_posX, scale), torch.tensor([0.01])).to_event(1))\n",
        "      posY = pyro.sample('posY', Normal(f_gumbel(N_posY), torch.tensor([0.01])).to_event(1))\n",
        "\n",
        "      #variables\n",
        "      latent = pyro.sample('latent', Normal(f_latent(N_latent, color, shape, scale, orien, posX, posY), torch.tensor([0.01])).to_event(1))\n",
        "      img = pyro.sample('img', Normal(f_image(N_img, latent, color, shape, scale, orien, posX, posY), torch.tensor([0.01])).to_event(1))\n",
        "\n",
        "      return torch.round(torch.cat([color, shape, scale, orien, posX, posY], -1)), latent, img\n",
        "    \n",
        "    def guide(noise = self.init_noise):\n",
        "      \n",
        "      #params\n",
        "      cat_val =  {'color': 1, 'shape': 3, 'scale': 6, 'orien': 40, 'posX': 32, 'posY': 32}\n",
        "      probs = {key : pyro.param('prob_'+key, tensor(1/value).repeat(value), constraint = constraints.positive) for key, value in cat_val.items()}\n",
        "      mu_latent = pyro.param('mu_latent', torch.ones(200)*0.5)\n",
        "      sigma_latent = pyro.param('sigma_latent', torch.ones(200)*0.2, constraint = constraints.positive)\n",
        "      low_img = pyro.param('low_img', torch.ones(4096)*0.5, constraint = constraints.interval(0., 1.))\n",
        "      high_img = pyro.param('high_img', torch.ones(4096)*0.02, constraint = constraints.interval(0., 1.))\n",
        "      \n",
        "      #noise variables\n",
        "      N_color = pyro.sample('N_color', OneHotCategorical(probs['color']))\n",
        "      N_shape = pyro.sample('N_shape', OneHotCategorical(probs['shape']))\n",
        "      N_scale = pyro.sample('N_scale', OneHotCategorical(probs['scale']))\n",
        "      N_orien = pyro.sample('N_orien', OneHotCategorical(probs['orien']))\n",
        "      N_posX = pyro.sample('N_posX', OneHotCategorical(probs['posX']))\n",
        "      N_posY = pyro.sample('N_posY', OneHotCategorical(probs['posY']))\n",
        "      N_latent = pyro.sample('N_latent', Normal(mu_latent, sigma_latent).to_event(1))\n",
        "      N_img = pyro.sample('N_img', Normal(low_img, high_img).to_event(1))\n",
        "      #print(sum(N_img > 1))\n",
        "      #print(sum(N_img < 0))\n",
        "      #variables\n",
        "      color = pyro.sample('color', Normal(f_gumbel(N_color), tensor(0.01)).to_event(1), infer = {'is_auxiliary' : True})\n",
        "      shape = pyro.sample('shape', Normal(f_gumbel(N_shape), tensor(0.01)).to_event(1), infer = {'is_auxiliary' : True})\n",
        "      scale = pyro.sample('scale', Normal(f_gumbel(N_scale), tensor(0.01)).to_event(1), infer = {'is_auxiliary' : True})\n",
        "      orien = pyro.sample('orien', Normal(f_gumbel(N_orien), tensor(0.01)).to_event(1), infer = {'is_auxiliary' : True})\n",
        "      posX = pyro.sample('posX', Normal(f_gumbel(N_posX), tensor(0.01)).to_event(1), infer = {'is_auxiliary' : True})\n",
        "      posY = pyro.sample('posY', Normal(f_gumbel(N_posY), tensor(0.01)).to_event(1), infer = {'is_auxiliary' : True})\n",
        "      latent = pyro.sample('latent', Normal(f_latent(N_latent, color, shape, scale, orien, posX, posY), tensor(0.01)).to_event(1))\n",
        "      img = pyro.sample('img', Normal(f_image(N_img, latent, color, shape, scale, orien, posX, posY), tensor(0.01)).to_event(1))\n",
        "\n",
        "      return \n",
        "    \n",
        "    \n",
        "\n",
        "    def viz_model():\n",
        "      label, _, img = model()\n",
        "      label = label_from_dummy(label)\n",
        "      plt.imshow(img.detach().numpy().reshape(64, 64), cmap = 'Greys')\n",
        "      text = 'Color:0    Shape:'+str(int(label[1])) + '   Scale:' + str(int(label[2])) + '   Orien.:' + str(int(label[3])) + '   Pos.X:' + str(int(label[4])) + '   Pos.Y:' + str(int(label[5]))\n",
        "      plt.title(text)\n",
        "      plt.show()\n",
        "\n",
        "    self.model = model\n",
        "    self.guide = guide\n",
        "    self.viz_model = viz_model\n",
        "\n",
        "\n",
        "scm = SCM(vae)\n",
        "scm.viz_model()"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWsAAAEICAYAAACZJtWMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2SJOOoJ9iOV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6PwIXTl0pqq",
        "colab_type": "code",
        "outputId": "abd1c46b-b614-41e7-d97d-ae9c633a9376",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        }
      },
      "source": [
        "#condition algoritm\n",
        "obs = {'color': tensor([1.]),\n",
        "      'shape': tensor([0., 1., 0.]),\n",
        "      'orien': torch.zeros(40),\n",
        "      'posX': torch.zeros(32),\n",
        "      'posY': torch.zeros(32)}\n",
        "obs['orien'][17] = 1.\n",
        "obs['posY'][3] = 1.\n",
        "obs['posX'][4] = 1.\n",
        "obs['scale'] = tensor([1.0, 0., 0., 0., 0., 0.])\n",
        "condModel = pyro.condition(scm.model, data = obs)\n",
        "pyro.clear_param_store()\n",
        "\n",
        "var_shapes = {'color': 1, 'shape':3, 'scale':6, 'orien':40, 'posX':32, 'posY':32}\n",
        "\n",
        "#setting the SVI attributes\n",
        "adam_params = {'lr': 0.01}\n",
        "optmizer = pyro.optim.Adam(adam_params)\n",
        "elbo = pyro.infer.Trace_ELBO()\n",
        "svi = pyro.infer.SVI(condModel, scm.guide, optmizer, elbo)\n",
        "\n",
        "#training loop\n",
        "losses = []\n",
        "prob_samples = {key : [] for key in var_shapes.keys()}\n",
        "mu_samples = []\n",
        "sigma_samples = []\n",
        "low_samples = []\n",
        "high_samples = []\n",
        "\n",
        "for i in range(500):\n",
        "  losses.append(svi.step(scm.init_noise))\n",
        "  #saving samples\n",
        "  for key in var_shapes.keys():\n",
        "    prob_samples[key].append(pyro.param('prob_'+key))\n",
        "  mu_samples.append(pyro.param('mu_latent'))\n",
        "  sigma_samples.append(pyro.param('sigma_latent'))\n",
        "  low_samples.append(pyro.param('low_img'))\n",
        "  high_samples.append(pyro.param('high_img'))\n",
        "\n",
        "#calculating means\n",
        "for key in var_shapes.keys():\n",
        "  prob_samples[key] = torch.mean(torch.stack(prob_samples[key]), dim = 0)\n",
        "mu_samples = torch.mean(torch.stack(mu_samples), dim = 0)\n",
        "sigma_samples = torch.mean(torch.stack(sigma_samples), dim = 0)\n",
        "low_samples = torch.mean(torch.stack(low_samples),dim = 0)\n",
        "high_samples = torch.mean(torch.stack(high_samples), dim = 0)\n",
        "\n",
        "updated_noise = {}\n",
        "for key in var_shapes.keys():\n",
        "  updated_noise[key] = OneHotCategorical( prob_samples[key])\n",
        "updated_noise['latent'] = Normal(mu_samples, sigma_samples)\n",
        "updated_noise['img'] =  Normal(low_samples, high_samples)\n",
        "img_samples = []\n",
        "for _ in range(1000):\n",
        "  _, _, s = scm.model(updated_noise)\n",
        "  img_samples.append(s)\n",
        "img_samples = torch.mean(torch.stack(img_samples), dim = 0)\n",
        "print(prob_samples)"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'color': tensor([1.], grad_fn=<MeanBackward1>), 'shape': tensor([0.3325, 0.3336, 0.3344], grad_fn=<MeanBackward1>), 'scale': tensor([0.1682, 0.1675, 0.1670, 0.1668, 0.1662, 0.1661],\n",
            "       grad_fn=<MeanBackward1>), 'orien': tensor([0.0250, 0.0251, 0.0250, 0.0250, 0.0251, 0.0250, 0.0253, 0.0251, 0.0250,\n",
            "        0.0250, 0.0250, 0.0251, 0.0251, 0.0250, 0.0251, 0.0250, 0.0250, 0.0251,\n",
            "        0.0252, 0.0251, 0.0251, 0.0250, 0.0252, 0.0251, 0.0250, 0.0250, 0.0251,\n",
            "        0.0251, 0.0252, 0.0250, 0.0250, 0.0250, 0.0250, 0.0252, 0.0253, 0.0251,\n",
            "        0.0251, 0.0254, 0.0250, 0.0252], grad_fn=<MeanBackward1>), 'posX': tensor([0.0312, 0.0318, 0.0314, 0.0313, 0.0313, 0.0314, 0.0313, 0.0313, 0.0313,\n",
            "        0.0315, 0.0313, 0.0313, 0.0313, 0.0313, 0.0313, 0.0315, 0.0314, 0.0314,\n",
            "        0.0315, 0.0313, 0.0314, 0.0312, 0.0313, 0.0313, 0.0314, 0.0312, 0.0313,\n",
            "        0.0313, 0.0312, 0.0313, 0.0314, 0.0312], grad_fn=<MeanBackward1>), 'posY': tensor([0.0314, 0.0313, 0.0315, 0.0313, 0.0313, 0.0314, 0.0315, 0.0313, 0.0313,\n",
            "        0.0314, 0.0313, 0.0315, 0.0313, 0.0312, 0.0313, 0.0313, 0.0313, 0.0312,\n",
            "        0.0313, 0.0313, 0.0313, 0.0313, 0.0313, 0.0314, 0.0314, 0.0314, 0.0313,\n",
            "        0.0313, 0.0313, 0.0313, 0.0314, 0.0313], grad_fn=<MeanBackward1>)}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSwrPI7rY4xG",
        "colab_type": "code",
        "outputId": "da8cd9e5-37f2-4f77-e318-099cf2eb97aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "low_samples"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],\n",
              "       grad_fn=<MeanBackward1>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnv_nVomY407",
        "colab_type": "code",
        "outputId": "ab6166d0-5a0e-42fd-8c1b-d27775690218",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "high_samples"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True),\n",
              " tensor([1.0008, 1.0008, 1.0008,  ..., 1.0008, 1.0008, 1.0008],\n",
              "        requires_grad=True)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmTEJyyDVmXv",
        "colab_type": "code",
        "outputId": "1f57767c-8f42-4cab-edaa-6bdbbfbff90c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#initial_noise = {'A': OneHotCategorical(probs = tensor([0.3, 0.1, 0.6]))}\n",
        "initial_noise = {'A': Uniform(tensor(0.), tensor(1.))}\n",
        "\n",
        "def f_A(N):\n",
        "  return N\n",
        "\n",
        "def model(noise):\n",
        "  N_A = pyro.sample('N_A', noise['A'])\n",
        "  A = pyro.sample('A', pyro.distributions.Normal(f_A(N_A), tensor(0.01)))\n",
        "  return A\n",
        "\n",
        "def guide(noise):\n",
        "  #prob_A = pyro.param('A_probs', tensor([1/3]).repeat(3), constraint = constraints.positive)\n",
        "  #N_A = pyro.sample('N_A', OneHotCategorical(probs = prob_A))\n",
        "  mu_A = pyro.param('A_mu', tensor(0.4), constraint = constraints.interval(0., 1.))\n",
        "  sigma_A = pyro.param('A_sigma', tensor(0.2), constraint = constraints.interval(0., 1.))\n",
        "  pyro.sample('N_A', Normal(mu_A, sigma_A))\n",
        "\n",
        "condModel = pyro.condition(model, data = {'A': tensor(0.6)})\n",
        "pyro.clear_param_store()\n",
        "\n",
        "#setting the SVI attributes\n",
        "adam_params = {'lr': 0.01}\n",
        "optmizer = pyro.optim.Adam(adam_params)\n",
        "elbo = pyro.infer.Trace_ELBO()\n",
        "svi = pyro.infer.SVI(condModel, guide, optmizer, elbo)\n",
        "\n",
        "#training loop\n",
        "#prob_samples = []\n",
        "mu_samples = []\n",
        "sigma_samples = []\n",
        "losses = []\n",
        "for i in range(1000):\n",
        "  losses.append(svi.step(initial_noise))\n",
        "  #saving samples\n",
        "  #prob_samples.append(pyro.param('A_probs'))\n",
        "  mu_samples.append(pyro.param('A_mu'))\n",
        "  sigma_samples.append(pyro.param('A_sigma'))\n",
        "#prob_samples = torch.mean(torch.stack(prob_samples), dim = 0)\n",
        "mu_samples = torch.mean(torch.stack(mu_samples), dim = 0)\n",
        "sigma_samples = torch.mean(torch.stack(sigma_samples), dim = 0)\n",
        "\n",
        "updated_noise = {'A': Normal(mu_samples, sigma_samples)}\n",
        "A_samples = []\n",
        "for _ in range(1000):\n",
        "  s = model(updated_noise)\n",
        "  A_samples.append(s)\n",
        "A_samples = torch.mean(torch.stack(A_samples), dim = 0)\n",
        "\n",
        "A_samples      "
      ],
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.5870, grad_fn=<MeanBackward1>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 161
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZdHVj908ePF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5bb2351b-8d33-4697-d754-1e522a8cbe27"
      },
      "source": [
        "model(initial_noise)"
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.4050)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZTGiqba8jOR",
        "colab_type": "code",
        "outputId": "d2b7bb5c-32b5-4b7e-c928-757263069f3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model(updated_noise)"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.5733, grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 159
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reMN63QaTYZ4",
        "colab_type": "code",
        "outputId": "56c540ac-3cda-4381-f34a-716ff5c4e4a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(mu_samples, sigma_samples)"
      ],
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.5855, grad_fn=<MeanBackward1>) tensor(0.0671, grad_fn=<MeanBackward1>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qn1eELDF-raa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "small = 0\n",
        "large = 0\n",
        "for i in range(len(losses)):\n",
        "  if losses[i] < 3:\n",
        "    small += 1\n",
        "  else:\n",
        "    large +=1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyMvMleWeN5e",
        "colab_type": "code",
        "outputId": "414a78d3-5ac1-4e67-d983-7f59ed253ac1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(small, large)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "859 141\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdZej5nc2Yj6",
        "colab_type": "code",
        "outputId": "83025b8b-d099-4135-cba9-b5578a8da604",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        }
      },
      "source": [
        "plt.scatter(range(len(losses)), losses, s = 1)\n",
        "plt.ylim(0, 4)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n3S+7brsq0QEAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smEFU85X47NC",
        "colab_type": "code",
        "outputId": "0e6aa648-2d95-41e7-820d-99eb9cc60f41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "losses"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[10104.669674873352,\n",
              " 10059.394067287445,\n",
              " 1.2171294689178467,\n",
              " 10034.824903130531,\n",
              " 9905.481527805328,\n",
              " 9959.031890153885,\n",
              " 9855.651611566544,\n",
              " 9934.827205777168,\n",
              " 9953.732473134995,\n",
              " 10272.443922400475,\n",
              " 9860.217040777206,\n",
              " 10029.26952791214,\n",
              " 1.291888952255249,\n",
              " 10039.333820819855,\n",
              " 1.3072712421417236,\n",
              " 1.314287543296814,\n",
              " 1.3206193447113037,\n",
              " 9972.985225200653,\n",
              " 10042.868489980698,\n",
              " 1.3390775918960571,\n",
              " 9952.594079732895,\n",
              " 1.3513746857643127,\n",
              " 10021.673749446869,\n",
              " 1.3633298873901367,\n",
              " 10203.436838269234,\n",
              " 9914.71223950386,\n",
              " 9808.053092360497,\n",
              " 9797.758408784866,\n",
              " 10089.483631491661,\n",
              " 9800.75064754486,\n",
              " 10249.307907938957,\n",
              " 9969.912957310677,\n",
              " 9940.631183505058,\n",
              " 1.433724820613861,\n",
              " 9905.083116292953,\n",
              " 10281.948043942451,\n",
              " 1.455955445766449,\n",
              " 10108.490466117859,\n",
              " 1.4699680805206299,\n",
              " 1.4764153957366943,\n",
              " 1.4822546243667603,\n",
              " 1.4875444173812866,\n",
              " 1.4923377633094788,\n",
              " 1.4966819286346436,\n",
              " 1.500619888305664,\n",
              " 9863.27238035202,\n",
              " 9977.334928631783,\n",
              " 10075.256103754044,\n",
              " 1.5187665224075317,\n",
              " 1.5236778259277344,\n",
              " 10000.606251478195,\n",
              " 1.533139169216156,\n",
              " 1.5376771688461304,\n",
              " 9976.10096693039,\n",
              " 9966.241013407707,\n",
              " 9949.515278935432,\n",
              " 9791.134730100632,\n",
              " 10195.145173192024,\n",
              " 9986.928393483162,\n",
              " 1.5770738124847412,\n",
              " 10181.319507479668,\n",
              " 1.589840054512024,\n",
              " 1.5957707166671753,\n",
              " 10022.405434846878,\n",
              " 1.606907844543457,\n",
              " 1.6121276021003723,\n",
              " 9870.440887093544,\n",
              " 1.622038185596466,\n",
              " 1.6267286539077759,\n",
              " 10014.927050590515,\n",
              " 1.6358218789100647,\n",
              " 9972.983798742294,\n",
              " 1.6451825499534607,\n",
              " 1.649669110774994,\n",
              " 9801.932501196861,\n",
              " 1.6583393216133118,\n",
              " 10158.115990519524,\n",
              " 1.6673094034194946,\n",
              " 9986.155851960182,\n",
              " 1.676543653011322,\n",
              " 1.6809636950492859,\n",
              " 1.684946596622467,\n",
              " 1.6885368824005127,\n",
              " 9744.365275621414,\n",
              " 1.6956921219825745,\n",
              " 10060.1990827322,\n",
              " 9777.156293988228,\n",
              " 1.7082189917564392,\n",
              " 1.7125177383422852,\n",
              " 1.7163879871368408,\n",
              " 10053.488786816597,\n",
              " 1.72403883934021,\n",
              " 1.727789282798767,\n",
              " 1.7311681509017944,\n",
              " 1.7342132925987244,\n",
              " 10159.14427947998,\n",
              " 1.7404805421829224,\n",
              " 1.7436527013778687,\n",
              " 1.746510624885559,\n",
              " 9900.882049560547,\n",
              " 10105.456449985504,\n",
              " 1.7564685940742493,\n",
              " 9769.67083454132,\n",
              " 9917.754665732384,\n",
              " 9983.877377986908,\n",
              " 9997.199647188187,\n",
              " 1.7801400423049927,\n",
              " 9948.642088890076,\n",
              " 9963.390569448471,\n",
              " 1.796648383140564,\n",
              " 9941.455385923386,\n",
              " 1.8075690865516663,\n",
              " 9978.628466844559,\n",
              " 1.8180993497371674,\n",
              " 1.8229965567588806,\n",
              " 1.827390730381012,\n",
              " 10125.639335155487,\n",
              " 1.835805892944336,\n",
              " 9974.739179134369,\n",
              " 1.8443365693092346,\n",
              " 1.8483923077583313,\n",
              " 1.8520339727401733,\n",
              " 1.8553058505058289,\n",
              " 1.8582468628883362,\n",
              " 1.860891878604889,\n",
              " 1.8632716834545135,\n",
              " 1.8654135465621948,\n",
              " 1.8673421442508698,\n",
              " 1.869079202413559,\n",
              " 1.870644062757492,\n",
              " 9878.837206840515,\n",
              " 10168.026051998138,\n",
              " 1.8772092461585999,\n",
              " 1.8798477053642273,\n",
              " 1.8822190761566162,\n",
              " 1.8843514919281006,\n",
              " 9937.42848789692,\n",
              " 9976.31400513649,\n",
              " 1.8922079503536224,\n",
              " 1.8951709568500519,\n",
              " 1.8978325426578522,\n",
              " 10034.719025969505,\n",
              " 1.9032700955867767,\n",
              " 1.9060039222240448,\n",
              " 1.9084593653678894,\n",
              " 1.9106658399105072,\n",
              " 1.912649780511856,\n",
              " 1.9144341349601746,\n",
              " 1.9160394966602325,\n",
              " 1.917484611272812,\n",
              " 10158.187023758888,\n",
              " 1.9209140539169312,\n",
              " 1.9228277802467346,\n",
              " 1.9245492815971375,\n",
              " 1.9260985553264618,\n",
              " 1.9274929463863373,\n",
              " 1.9287486672401428,\n",
              " 1.9298793971538544,\n",
              " 1.9308982491493225,\n",
              " 1.9318161308765411,\n",
              " 1.9326432645320892,\n",
              " 9743.593604683876,\n",
              " 1.934985637664795,\n",
              " 9918.197422027588,\n",
              " 1.938615769147873,\n",
              " 1.940587192773819,\n",
              " 1.9423596858978271,\n",
              " 1.9439537227153778,\n",
              " 1.9453878998756409,\n",
              " 9983.123681902885,\n",
              " 10038.119692444801,\n",
              " 1.951454758644104,\n",
              " 1.9538928270339966,\n",
              " 1.956080287694931,\n",
              " 10078.156422615051,\n",
              " 1.960722953081131,\n",
              " 1.9631262421607971,\n",
              " 10040.764255166054,\n",
              " 1.968110203742981,\n",
              " 1.970644474029541,\n",
              " 1.9729179739952087,\n",
              " 9976.888409137726,\n",
              " 1.9776557087898254,\n",
              " 1.9800731539726257,\n",
              " 1.9822413623332977,\n",
              " 1.9841870665550232,\n",
              " 1.9859341382980347,\n",
              " 1.9875038266181946,\n",
              " 10033.30269742012,\n",
              " 1.9910264313220978,\n",
              " 1.992922157049179,\n",
              " 9961.258133530617,\n",
              " 1.9969958066940308,\n",
              " 1.9991220831871033,\n",
              " 2.001029998064041,\n",
              " 10023.423968791962,\n",
              " 2.00510773062706,\n",
              " 2.0072271823883057,\n",
              " 10067.378394842148,\n",
              " 2.011641174554825,\n",
              " 2.0138913989067078,\n",
              " 9998.06609749794,\n",
              " 2.0185112059116364,\n",
              " 2.020842969417572,\n",
              " 2.022932857275009,\n",
              " 2.024807095527649,\n",
              " 2.026489019393921,\n",
              " 2.027999073266983,\n",
              " 2.0293556451797485,\n",
              " 2.030574530363083,\n",
              " 2.031670331954956,\n",
              " 2.032655894756317,\n",
              " 2.0335423350334167,\n",
              " 10212.912897825241,\n",
              " 10095.4380133152,\n",
              " 2.0379894971847534,\n",
              " 2.0398974120616913,\n",
              " 2.041607230901718,\n",
              " 2.043140560388565,\n",
              " 2.0445162057876587,\n",
              " 2.045751303434372,\n",
              " 10150.96153640747,\n",
              " 2.0486409068107605,\n",
              " 2.0502375960350037,\n",
              " 2.051670730113983,\n",
              " 10021.721795797348,\n",
              " 2.0548480451107025,\n",
              " 2.0565425008535385,\n",
              " 9894.969330310822,\n",
              " 10253.841989040375,\n",
              " 2.0627285093069077,\n",
              " 10154.789731025696,\n",
              " 2.0678436309099197,\n",
              " 2.0703380554914474,\n",
              " 2.072567954659462,\n",
              " 2.074563056230545,\n",
              " 9956.130044698715,\n",
              " 2.078630566596985,\n",
              " 2.080669730901718,\n",
              " 2.0824946463108063,\n",
              " 9701.718229055405,\n",
              " 2.0862364023923874,\n",
              " 2.088120698928833,\n",
              " 2.089806944131851,\n",
              " 2.091316983103752,\n",
              " 2.092670202255249,\n",
              " 2.093883827328682,\n",
              " 2.094972535967827,\n",
              " 2.0959499925374985,\n",
              " 9864.506886720657,\n",
              " 2.098260447382927,\n",
              " 2.099544942378998,\n",
              " 2.100697383284569,\n",
              " 2.101731702685356,\n",
              " 10146.230481147766,\n",
              " 2.1041383892297745,\n",
              " 2.1054624766111374,\n",
              " 2.1066495180130005,\n",
              " 2.107714354991913,\n",
              " 2.1086701154708862,\n",
              " 2.109528213739395,\n",
              " 2.1102991849184036,\n",
              " 2.110991895198822,\n",
              " 2.1116146445274353,\n",
              " 2.1121746301651,\n",
              " 2.1126783937215805,\n",
              " 2.113131672143936,\n",
              " 2.113539546728134,\n",
              " 2.113906502723694,\n",
              " 2.114237055182457,\n",
              " 2.114534631371498,\n",
              " 2.1148025542497635,\n",
              " 2.115043818950653,\n",
              " 2.11526121199131,\n",
              " 2.1154569387435913,\n",
              " 9976.630516052246,\n",
              " 2.1164318174123764,\n",
              " 2.117148905992508,\n",
              " 10028.849713802338,\n",
              " 10171.819626808167,\n",
              " 2.120689883828163,\n",
              " 2.1222075074911118,\n",
              " 9783.578895568848,\n",
              " 2.1253590434789658,\n",
              " 2.126961886882782,\n",
              " 2.1283959597349167,\n",
              " 2.1296799778938293,\n",
              " 9989.633150577545,\n",
              " 9804.688898086548,\n",
              " 2.1344261318445206,\n",
              " 2.136199250817299,\n",
              " 2.137782871723175,\n",
              " 2.139198422431946,\n",
              " 2.1404650062322617,\n",
              " 2.1415989249944687,\n",
              " 2.1426148861646652,\n",
              " 2.14352585375309,\n",
              " 2.144342854619026,\n",
              " 2.1450761556625366,\n",
              " 2.145734652876854,\n",
              " 10032.09111046791,\n",
              " 2.1473964750766754,\n",
              " 2.1483553797006607,\n",
              " 2.1492147892713547,\n",
              " 2.149985432624817,\n",
              " 2.1506770700216293,\n",
              " 2.151297852396965,\n",
              " 2.151855394244194,\n",
              " 2.1523563265800476,\n",
              " 2.152806520462036,\n",
              " 2.1532112509012222,\n",
              " 9934.83058643341,\n",
              " 2.1544384509325027,\n",
              " 2.1552127450704575,\n",
              " 2.155907467007637,\n",
              " 2.1565312892198563,\n",
              " 2.157091721892357,\n",
              " 9845.672257184982,\n",
              " 2.158562421798706,\n",
              " 2.1594289988279343,\n",
              " 2.1602058857679367,\n",
              " 2.1609026491642,\n",
              " 2.1615279018878937,\n",
              " 2.162089303135872,\n",
              " 2.162593424320221,\n",
              " 2.1630463749170303,\n",
              " 2.1634535640478134,\n",
              " 2.1638196408748627,\n",
              " 2.164148822426796,\n",
              " 2.164444923400879,\n",
              " 10007.537953853607,\n",
              " 2.1654574424028397,\n",
              " 2.166126638650894,\n",
              " 2.16672720015049,\n",
              " 2.1672665178775787,\n",
              " 2.167750984430313,\n",
              " 2.1681862622499466,\n",
              " 2.1685775369405746,\n",
              " 2.1689292639493942,\n",
              " 2.1692456752061844,\n",
              " 2.1695303171873093,\n",
              " 2.169786363840103,\n",
              " 2.1700166910886765,\n",
              " 9764.393394708633,\n",
              " 2.1709007173776627,\n",
              " 2.171507865190506,\n",
              " 2.1720527559518814,\n",
              " 2.1725421100854874,\n",
              " 2.1729817539453506,\n",
              " 2.173376649618149,\n",
              " 2.173731714487076,\n",
              " 2.174050882458687,\n",
              " 2.1743380278348923,\n",
              " 2.174596428871155,\n",
              " 2.174828752875328,\n",
              " 2.1750379502773285,\n",
              " 10053.585408210754,\n",
              " 2.175884887576103,\n",
              " 2.1764758825302124,\n",
              " 2.17700631916523,\n",
              " 10040.905223369598,\n",
              " 10138.797328472137,\n",
              " 2.1796731874346733,\n",
              " 2.1808178648352623,\n",
              " 10220.22947716713,\n",
              " 2.1832096725702286,\n",
              " 2.1844304725527763,\n",
              " 2.1855203360319138,\n",
              " 2.186494544148445,\n",
              " 2.1873658150434494,\n",
              " 2.1881457045674324,\n",
              " 2.188844256103039,\n",
              " 2.1894703656435013,\n",
              " 2.190031982958317,\n",
              " 10204.094831228256,\n",
              " 2.1914336159825325,\n",
              " 2.1922370344400406,\n",
              " 2.1929565370082855,\n",
              " 2.1936013475060463,\n",
              " 2.1941793859004974,\n",
              " 2.1946978643536568,\n",
              " 10161.008487462997,\n",
              " 2.195997767150402,\n",
              " 2.19674451649189,\n",
              " 2.1974133774638176,\n",
              " 2.1980127096176147,\n",
              " 2.198550172150135,\n",
              " 2.19903215020895,\n",
              " 2.199464924633503,\n",
              " 9997.136769533157,\n",
              " 2.2006107419729233,\n",
              " 2.2012888342142105,\n",
              " 10019.943745136261,\n",
              " 2.2028264701366425,\n",
              " 2.203657701611519,\n",
              " 2.204401396214962,\n",
              " 2.205067127943039,\n",
              " 9746.953139781952,\n",
              " 2.20657180249691,\n",
              " 2.2073836103081703,\n",
              " 2.208109602332115,\n",
              " 2.208759382367134,\n",
              " 2.2093413546681404,\n",
              " 2.20986295491457,\n",
              " 2.210330620408058,\n",
              " 2.210750237107277,\n",
              " 2.2111268416047096,\n",
              " 2.21146497130394,\n",
              " 2.211768724024296,\n",
              " 2.212041787803173,\n",
              " 2.212287038564682,\n",
              " 2.2125075832009315,\n",
              " 2.212705910205841,\n",
              " 10024.049201011658,\n",
              " 2.2134034112095833,\n",
              " 2.2138686403632164,\n",
              " 2.2142858058214188,\n",
              " 2.214660033583641,\n",
              " 2.2149960696697235,\n",
              " 2.21529770642519,\n",
              " 2.2155686989426613,\n",
              " 2.2158122435212135,\n",
              " 2.216031104326248,\n",
              " 2.216227948665619,\n",
              " 2.216404877603054,\n",
              " 2.216564103960991,\n",
              " 9921.581701993942,\n",
              " 9785.923183918,\n",
              " 2.2179579958319664,\n",
              " 2.2186428382992744,\n",
              " 2.219255492091179,\n",
              " 2.219803936779499,\n",
              " 2.2202952578663826,\n",
              " 2.220735751092434,\n",
              " 2.2211306616663933,\n",
              " 2.2214851453900337,\n",
              " 2.2218032553792,\n",
              " 2.2220890298485756,\n",
              " 2.2223456501960754,\n",
              " 2.2225763872265816,\n",
              " 2.2227835655212402,\n",
              " 9952.614638328552,\n",
              " 2.2234590500593185,\n",
              " 2.2238971889019012,\n",
              " 2.224289946258068,\n",
              " 2.2246420234441757,\n",
              " 2.224957972764969,\n",
              " 2.2252416163682938,\n",
              " 9986.568258285522,\n",
              " 2.226035326719284,\n",
              " 9933.690511226654,\n",
              " 2.227268733084202,\n",
              " 2.227939583361149,\n",
              " 2.228539191186428,\n",
              " 2.229075640439987,\n",
              " 2.2295558899641037,\n",
              " 2.229986049234867,\n",
              " 2.230371616780758,\n",
              " 2.2307174652814865,\n",
              " 2.231027714908123,\n",
              " 2.2313061952590942,\n",
              " 2.2315563037991524,\n",
              " 2.231781020760536,\n",
              " 2.2319828495383263,\n",
              " 2.2321643009781837,\n",
              " 2.2323274984955788,\n",
              " 2.23247417062521,\n",
              " 2.2326061874628067,\n",
              " 2.2327248230576515,\n",
              " 2.232831686735153,\n",
              " 2.232927806675434,\n",
              " 2.233014263212681,\n",
              " 2.2330921590328217,\n",
              " 2.2331621795892715,\n",
              " 2.233225367963314,\n",
              " 2.233282096683979,\n",
              " 2.2333333268761635,\n",
              " 2.2333793863654137,\n",
              " 2.2334208339452744,\n",
              " 2.2334582060575485,\n",
              " 2.233491860330105,\n",
              " 2.23352213203907,\n",
              " 2.2335494682192802,\n",
              " 2.23357405513525,\n",
              " 2.233596086502075,\n",
              " 2.2336160764098167,\n",
              " 2.2336340844631195,\n",
              " 2.2336502447724342,\n",
              " 2.2336648032069206,\n",
              " 2.233677960932255,\n",
              " 2.233689710497856,\n",
              " 9851.295250415802,\n",
              " 2.234016850590706,\n",
              " 2.2343005537986755,\n",
              " 2.2345551550388336,\n",
              " 2.234783723950386,\n",
              " 2.2349889054894447,\n",
              " 2.23517332226038,\n",
              " 2.2353388965129852,\n",
              " 2.23548773676157,\n",
              " 2.235621653497219,\n",
              " 2.235741972923279,\n",
              " 2.2358501702547073,\n",
              " 2.2359475940465927,\n",
              " 2.2360351905226707,\n",
              " 2.2361139953136444,\n",
              " 2.236184887588024,\n",
              " 2.2362487763166428,\n",
              " 2.2363062873482704,\n",
              " 2.236358068883419,\n",
              " 2.2364045530557632,\n",
              " 2.2364465817809105,\n",
              " 2.2364843487739563,\n",
              " 2.2365183532238007,\n",
              " 2.2365489155054092,\n",
              " 2.236576482653618,\n",
              " 2.2366013154387474,\n",
              " 2.236623726785183,\n",
              " 2.23664378374815,\n",
              " 2.2366620525717735,\n",
              " 10140.956403970718,\n",
              " 2.236990138888359,\n",
              " 2.2372697591781616,\n",
              " 2.237520545721054,\n",
              " 2.2377456426620483,\n",
              " 2.2379476726055145,\n",
              " 2.2381291911005974,\n",
              " 2.2382922396063805,\n",
              " 2.2384387403726578,\n",
              " 2.2385705411434174,\n",
              " 2.2386889830231667,\n",
              " 2.2387955337762833,\n",
              " 2.2388913333415985,\n",
              " 2.238977462053299,\n",
              " 2.239055022597313,\n",
              " 2.239124819636345,\n",
              " 2.2391876950860023,\n",
              " 9978.602709054947,\n",
              " 2.2395770996809006,\n",
              " 2.2398753464221954,\n",
              " 2.240142807364464,\n",
              " 2.2403826005756855,\n",
              " 2.2405977956950665,\n",
              " 2.2407910712063313,\n",
              " 2.2409645915031433,\n",
              " 2.2411204613745213,\n",
              " 2.241260528564453,\n",
              " 10058.832080125809,\n",
              " 2.2417932376265526,\n",
              " 2.242157507687807,\n",
              " 2.2424838542938232,\n",
              " 2.242776434868574,\n",
              " 2.2430388145148754,\n",
              " 2.2432742454111576,\n",
              " 2.2434855960309505,\n",
              " 2.2436754032969475,\n",
              " 2.2438459619879723,\n",
              " 2.243999108672142,\n",
              " 2.2441367506980896,\n",
              " 2.244260534644127,\n",
              " 2.244371797889471,\n",
              " 10204.076253652573,\n",
              " 2.2448457702994347,\n",
              " 2.245180331170559,\n",
              " 2.2454800121486187,\n",
              " 10030.130168676376,\n",
              " 2.2462461553514004,\n",
              " 2.2466908134520054,\n",
              " 2.247088525444269,\n",
              " 2.2474446333944798,\n",
              " 2.247763529419899,\n",
              " 2.248049344867468,\n",
              " 2.2483056262135506,\n",
              " 2.248535353690386,\n",
              " 2.2487416937947273,\n",
              " 2.2489268593490124,\n",
              " 9950.13784980774,\n",
              " 2.2494839765131474,\n",
              " 9935.792768716812,\n",
              " 2.2503947727382183,\n",
              " 2.2508957758545876,\n",
              " 2.2513432018458843,\n",
              " 2.2517430186271667,\n",
              " 2.252100609242916,\n",
              " 2.2524207830429077,\n",
              " 2.2527075223624706,\n",
              " 2.2529644928872585,\n",
              " 2.253194909542799,\n",
              " 2.253401666879654,\n",
              " 2.2535872235894203,\n",
              " 2.2537537813186646,\n",
              " 2.253903478384018,\n",
              " 2.2540378905832767,\n",
              " 2.254158642143011,\n",
              " 2.254267305135727,\n",
              " 2.254364885389805,\n",
              " 2.2544527016580105,\n",
              " 2.254531566053629,\n",
              " 10033.33032822609,\n",
              " 2.2548885978758335,\n",
              " 2.2551449462771416,\n",
              " 2.2553744204342365,\n",
              " 2.2555801048874855,\n",
              " 2.2557647600769997,\n",
              " 2.255930330604315,\n",
              " 2.2560790181159973,\n",
              " 2.2562126964330673,\n",
              " 2.2563326247036457,\n",
              " 2.256440430879593,\n",
              " 2.256537362933159,\n",
              " 2.256624549627304,\n",
              " 2.256702806800604,\n",
              " 2.2567733824253082,\n",
              " 2.2568366527557373,\n",
              " 2.256893679499626,\n",
              " 2.25694502517581,\n",
              " 2.2569911293685436,\n",
              " 2.257032800465822,\n",
              " 2.257070168852806,\n",
              " 2.2571037895977497,\n",
              " 2.2571341060101986,\n",
              " 2.2571614906191826,\n",
              " 2.2571859396994114,\n",
              " 2.2572080828249454,\n",
              " 2.2572279796004295,\n",
              " 2.2572458796203136,\n",
              " 2.257262095808983,\n",
              " 2.257276687771082,\n",
              " 9823.806645154953,\n",
              " 10050.116183280945,\n",
              " 2.257936254143715,\n",
              " 2.258315935730934,\n",
              " 2.2586553655564785,\n",
              " 2.2589588165283203,\n",
              " 2.2592306807637215,\n",
              " 2.259474031627178,\n",
              " 2.2596920616924763,\n",
              " 2.259887717664242,\n",
              " 2.2600633054971695,\n",
              " 2.260220766067505,\n",
              " 2.2603621631860733,\n",
              " 2.2604891806840897,\n",
              " 2.2606033124029636,\n",
              " 2.2607058100402355,\n",
              " 10117.668999195099,\n",
              " 2.261094693094492,\n",
              " 2.2613601982593536,\n",
              " 2.261597752571106,\n",
              " 2.2618104815483093,\n",
              " 2.2620013132691383,\n",
              " 2.262172307819128,\n",
              " 2.2623257786035538,\n",
              " 2.262463465332985,\n",
              " 2.2625870518386364,\n",
              " 2.2626980915665627,\n",
              " 2.2627978920936584,\n",
              " 2.262887515127659,\n",
              " 2.262968070805073,\n",
              " 2.263040564954281,\n",
              " 2.263105731457472,\n",
              " 2.263164386153221,\n",
              " 2.2632169611752033,\n",
              " 2.263264514505863,\n",
              " 2.2633071057498455,\n",
              " 2.2633454762399197,\n",
              " 2.263380005955696,\n",
              " 2.263411182910204,\n",
              " 2.2634392008185387,\n",
              " 2.26346442848444,\n",
              " 2.2634870521724224,\n",
              " 2.263507504016161,\n",
              " 2.263525914400816,\n",
              " 2.2635424621403217,\n",
              " 2.2635574601590633,\n",
              " 2.263570785522461,\n",
              " 2.263582933694124,\n",
              " 2.263593714684248,\n",
              " 2.2636036947369576,\n",
              " 2.2636123709380627,\n",
              " 2.263620365411043,\n",
              " 2.263627551496029,\n",
              " 2.2636339999735355,\n",
              " 2.2636397629976273,\n",
              " 2.263644967228174,\n",
              " 2.2636496759951115,\n",
              " 2.263653952628374,\n",
              " 2.263657733798027,\n",
              " 2.263661202043295,\n",
              " 2.2636643014848232,\n",
              " 2.2636670917272568,\n",
              " 2.2636695690453053,\n",
              " 2.263671800494194,\n",
              " 2.263673909008503,\n",
              " 2.263675704598427,\n",
              " 10079.890282392502,\n",
              " 2.263885386288166,\n",
              " 2.264071673154831,\n",
              " 2.26423866301775,\n",
              " 2.264388471841812,\n",
              " 2.2645227164030075,\n",
              " 2.26464332267642,\n",
              " 2.264751598238945,\n",
              " 2.264848906546831,\n",
              " 2.264936238527298,\n",
              " 2.265014711767435,\n",
              " 2.2650851979851723,\n",
              " 2.265148751437664,\n",
              " 2.2652057372033596,\n",
              " 2.265257153660059,\n",
              " 2.265303187072277,\n",
              " 2.265344761312008,\n",
              " 2.2653820663690567,\n",
              " 2.265415720641613,\n",
              " 2.2654459699988365,\n",
              " 2.265473186969757,\n",
              " 2.2654976211488247,\n",
              " 2.265519704669714,\n",
              " 2.2655395604670048,\n",
              " 2.265557497739792,\n",
              " 10066.98591542244,\n",
              " 2.265785463154316,\n",
              " 2.2659751661121845,\n",
              " 2.2661450505256653,\n",
              " 2.2662973552942276,\n",
              " 2.2664340026676655,\n",
              " 2.266556542366743,\n",
              " 2.2666665874421597,\n",
              " 2.2667653746902943,\n",
              " 2.2668540254235268,\n",
              " 2.2669337056577206,\n",
              " 2.267005417495966,\n",
              " 2.267069771885872,\n",
              " 2.267127700150013,\n",
              " 2.267179761081934,\n",
              " 2.267226628959179,\n",
              " 2.267268743366003,\n",
              " 2.2673066556453705,\n",
              " 2.267340674996376,\n",
              " 2.2673714235424995,\n",
              " 2.2673990204930305,\n",
              " 2.267423901706934,\n",
              " 2.2674462497234344,\n",
              " 2.267466500401497,\n",
              " 2.267484463751316,\n",
              " 2.2675008848309517,\n",
              " 2.267515577375889,\n",
              " 2.2675288505852222,\n",
              " 2.2675407640635967,\n",
              " 2.2675514444708824,\n",
              " 2.267561011016369,\n",
              " 2.267569839954376,\n",
              " 2.267577677965164,\n",
              " 2.267584715038538,\n",
              " 2.2675910107791424,\n",
              " 10165.96797657013,\n",
              " 2.2677915431559086,\n",
              " 2.2679660581052303,\n",
              " 2.2681224048137665,\n",
              " 2.268262565135956,\n",
              " 2.268388271331787,\n",
              " 2.268501076847315,\n",
              " 2.2686023339629173,\n",
              " 2.268693223595619,\n",
              " 10076.04483127594,\n",
              " 2.26902724057436,\n",
              " 2.269252687692642,\n",
              " 2.2694543674588203,\n",
              " 2.2696347534656525,\n",
              " 2.269796382635832,\n",
              " 2.269941173493862,\n",
              " 2.270071040838957,\n",
              " 2.2701874114573,\n",
              " 2.27029200643301,\n",
              " 2.2703858204185963,\n",
              " 2.2704700864851475,\n",
              " 2.2705457285046577,\n",
              " 2.2706136740744114,\n",
              " 2.270674780011177,\n",
              " 2.2707297317683697,\n",
              " 2.2707791440188885,\n",
              " 2.2708235681056976,\n",
              " 2.270863499492407,\n",
              " 2.2708994299173355,\n",
              " 2.270931851118803,\n",
              " 2.2709608897566795,\n",
              " 2.270987097173929,\n",
              " 2.2710106559097767,\n",
              " 2.2710318192839622,\n",
              " 2.271050889045,\n",
              " 2.271068174391985,\n",
              " 2.2710836119949818,\n",
              " 10337.607775211334,\n",
              " 2.2712828926742077,\n",
              " 2.2714488059282303,\n",
              " 2.2715973537415266,\n",
              " 2.2717305123806,\n",
              " 2.2718499451875687,\n",
              " 2.2719570733606815,\n",
              " 2.272053189575672,\n",
              " 2.2721394654363394,\n",
              " 2.272217009216547,\n",
              " 2.272286619991064,\n",
              " 2.272349163889885,\n",
              " 2.2724053729325533,\n",
              " 2.272455869242549,\n",
              " 2.2725013848394156,\n",
              " 2.272542232647538,\n",
              " 2.272578962147236,\n",
              " 2.2726120054721832,\n",
              " 2.2726417928934097,\n",
              " 2.272668508812785,\n",
              " 2.272692644968629,\n",
              " 2.2727143857628107,\n",
              " 2.2727337907999754,\n",
              " 2.2727514151483774,\n",
              " 9973.296827316284,\n",
              " 2.272952204570174,\n",
              " 2.273117719218135,\n",
              " 2.2732660192996264,\n",
              " 2.273398833349347,\n",
              " 2.273518005385995,\n",
              " 2.2736248932778835,\n",
              " 2.2737207263708115,\n",
              " 2.2738067973405123,\n",
              " 2.2738840896636248,\n",
              " 2.2739535234868526,\n",
              " 2.2740159612149,\n",
              " 2.2740718945860863,\n",
              " 2.274122428148985,\n",
              " 2.274167686700821,\n",
              " 2.274208465591073,\n",
              " 2.2742451950907707,\n",
              " 2.274278122931719,\n",
              " 2.2743077985942364,\n",
              " 2.2743344083428383,\n",
              " 2.274358442053199,\n",
              " 2.274380085989833,\n",
              " 2.27439958229661,\n",
              " 2.2744170539081097,\n",
              " 2.274432810023427,\n",
              " 2.2744469717144966,\n",
              " 2.274459784850478,\n",
              " 2.2744713108986616,\n",
              " 2.2744816709309816,\n",
              " 2.274490926414728,\n",
              " 2.274499386548996,\n",
              " 9909.081001758575,\n",
              " 2.274666605517268,\n",
              " 2.274809530004859,\n",
              " 2.274937603622675,\n",
              " 2.2750524263828993,\n",
              " 2.2751553524285555,\n",
              " 2.2752477321773767,\n",
              " 2.2753305472433567,\n",
              " 2.275405088439584,\n",
              " 2.275471905246377,\n",
              " 2.27553204447031,\n",
              " 2.2755859941244125,\n",
              " 2.275634491816163,\n",
              " 9966.644718170166,\n",
              " 2.275874190032482,\n",
              " 2.2760496269911528,\n",
              " 2.27620661072433,\n",
              " 2.2763471733778715,\n",
              " 2.27647309191525,\n",
              " 2.276586027815938,\n",
              " 2.276687266305089,\n",
              " 2.276778157800436,\n",
              " 2.2768597453832626,\n",
              " 2.2769329492002726,\n",
              " 2.2769987490028143,\n",
              " 2.2770578786730766,\n",
              " 2.2771109510213137,\n",
              " 2.2771587632596493,\n",
              " 2.277201682329178,\n",
              " 2.2772402595728636,\n",
              " 2.2772750444710255,\n",
              " 2.2773062214255333,\n",
              " 2.2773343417793512,\n",
              " 2.2773595862090588,\n",
              " 2.2773823868483305,\n",
              " 2.277402864769101,\n",
              " 2.27742126211524,\n",
              " 2.277437826618552,\n",
              " 2.277452802285552,\n",
              " 2.2774662487208843,\n",
              " 2.277478288859129,\n",
              " 2.2774892300367355,\n",
              " 2.277499008923769,\n",
              " 2.277507809922099,\n",
              " 2.2775158770382404,\n",
              " 2.2775229066610336,\n",
              " 2.2775293849408627,\n",
              " 2.277535190805793,\n",
              " 2.277540385723114,\n",
              " 2.277545152232051,\n",
              " 2.2775494307279587,\n",
              " 2.277553219348192,\n",
              " 2.2775565814226866,\n",
              " 2.2775596976280212,\n",
              " 2.2775625083595514,\n",
              " 2.2775650154799223,\n",
              " 2.2775672767311335,\n",
              " 2.277569292113185,\n",
              " 2.277571126818657,\n",
              " 2.277572715654969,\n",
              " 2.2775743044912815,\n",
              " 2.2775755878537893,\n",
              " 2.2775768097490072,\n",
              " 2.2775778491050005,\n",
              " 2.277578826993704,\n",
              " 2.2775797434151173,\n",
              " 2.277580477297306,\n",
              " 9943.650945663452,\n",
              " 2.277731904760003,\n",
              " 2.2778668142855167,\n",
              " 2.2779876571148634,\n",
              " 2.2780958395451307,\n",
              " 2.2781928945332766,\n",
              " 2.2782799247652292,\n",
              " 2.278357969596982,\n",
              " 2.2784280721098185,\n",
              " 2.278491023927927,\n",
              " 2.278547501191497,\n",
              " 2.2785983588546515,\n",
              " 2.278643963858485,\n",
              " 9911.122983455658,\n",
              " 2.278858771547675,\n",
              " 2.2790141571313143,\n",
              " 2.279153287410736,\n",
              " 2.279277816414833,\n",
              " 2.2793894596397877,\n",
              " 2.2794894985854626,\n",
              " 9849.85652923584,\n",
              " 2.27978846244514,\n",
              " 2.279975162819028,\n",
              " 2.280141895636916,\n",
              " 2.280290925875306,\n",
              " 2.2804242111742496,\n",
              " 2.2805435862392187,\n",
              " 2.280650395900011,\n",
              " 2.280746167525649,\n",
              " 2.280832063406706,\n",
              " 2.2809090558439493,\n",
              " 2.280978187918663,\n",
              " 2.2810402493923903,\n",
              " 2.2810960356146097,\n",
              " 2.2811460327357054,\n",
              " 2.281190972775221,\n",
              " 2.281231466680765,\n",
              " 2.2812677565962076,\n",
              " 2.281300514936447,\n",
              " 2.2813298609107733,\n",
              " 2.2813562843948603,\n",
              " 2.2813800908625126,\n",
              " 2.2814014591276646,\n",
              " 2.2814207579940557,\n",
              " 2.281437987461686,\n",
              " 2.2814535722136497,\n",
              " 2.2814676351845264,\n",
              " 2.281480235978961,\n",
              " 2.281491558998823,\n",
              " 2.2815018463879824,\n",
              " 2.281511038541794,\n",
              " 2.281519317999482,\n",
              " 2.281526805832982,\n",
              " 2.2815335616469383,\n",
              " 2.2815395891666412,\n",
              " 2.281545005738735,\n",
              " 2.281549936160445,\n",
              " 2.2815543804317713,\n",
              " 2.2815582770854235,\n",
              " 2.281561868265271,\n",
              " 2.2815650943666697,\n",
              " 2.281567955389619,\n",
              " 2.2815706934779882,\n",
              " 2.2815730068832636,\n",
              " 2.2815750148147345,\n",
              " 2.2815769631415606,\n",
              " 2.281578728929162,\n",
              " 2.281580312177539,\n",
              " 2.281581651419401,\n",
              " 2.2815829291939735,\n",
              " 2.2815840244293213,\n",
              " 2.2815850600600243,\n",
              " 2.2815859727561474,\n",
              " 2.281586764380336,\n",
              " 2.281587554141879,\n",
              " 2.281588163226843,\n",
              " 2.2815887723118067,\n",
              " 2.2815893199294806,\n",
              " 2.2815898079425097,\n",
              " 2.2815902940928936,\n",
              " 2.281590659171343,\n",
              " 2.281590962782502,\n",
              " 2.2815912682563066,\n",
              " 2.281591571867466,\n",
              " 2.2815918158739805,\n",
              " 2.28159211948514,\n",
              " 2.2815923020243645]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2Qy2R4p-5t8",
        "colab_type": "code",
        "outputId": "e3a0d252-8ce9-439d-bbe4-0da3f106370a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "source": [
        "Uniform(tensor(-1.), tensor(-2.)).sample()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-125-f65a1220e660>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mUniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/distributions/uniform.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, low, high, validate_args)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_args\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhigh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Uniform is not defined when low>= high\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_instance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Uniform is not defined when low>= high"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvZ7DJFkZJ-h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "initial_noise = {'A': Uniform(tensor(0.), tensor(1.))}\n",
        "\n",
        "def f_A(N):\n",
        "  return N\n",
        "\n",
        "def model(noise):\n",
        "  N_A = pyro.sample('N_A', noise['A'])\n",
        "  A = pyro.sample('A', pyro.distributions.Normal(f_A(N_A), tensor(0.01)).to_event(1))\n",
        "  return A\n",
        "\n",
        "def guide(noise):\n",
        "  mu_A = pyro.param('A_probs', tensor([1/3]).repeat(3), constraint = constraints.positive)\n",
        "  \n",
        "  N_A = pyro.sample('N_A', OneHotCategorical(probs = prob_A))\n",
        "\n",
        "\n",
        "condModel = pyro.condition(model, data = {'A': tensor([0., 1., 0.])})\n",
        "pyro.clear_param_store()\n",
        "\n",
        "#setting the SVI attributes\n",
        "adam_params = {'lr': 0.01}\n",
        "optmizer = pyro.optim.Adam(adam_params)\n",
        "elbo = pyro.infer.Trace_ELBO()\n",
        "svi = pyro.infer.SVI(condModel, guide, optmizer, elbo)\n",
        "\n",
        "#training loop\n",
        "prob_samples = []\n",
        "losses = []\n",
        "for i in range(1000):\n",
        "  losses.append(svi.step(initial_noise))\n",
        "  #saving samples\n",
        "  prob_samples.append(pyro.param('A_probs'))\n",
        "\n",
        "prob_samples = torch.mean(torch.stack(prob_samples), dim = 0)\n",
        "\n",
        "updated_noise = {'A': OneHotCategorical(prob_samples)}\n",
        "A_samples = []\n",
        "for _ in range(1000):\n",
        "  s = model(updated_noise)\n",
        "  A_samples.append(s)\n",
        "A_samples = torch.mean(torch.stack(A_samples), dim = 0)\n",
        "\n",
        "A_samples      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKoIjkZj9pQ1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a55c4ead-e972-4182-c297-8a07a8f45106"
      },
      "source": [
        "def t_mean(tens):\n",
        "  return torch.mean(torch.stack(tens), dim = 0)\n",
        "\n",
        "class SCM_test():\n",
        "  def __init__(self, vae):\n",
        "    self.vae = vae\n",
        "    self.latents_names = ['shape', 'scale', 'orien', 'posX', 'posY']\n",
        "    dist = OneHotCategorical\n",
        "    self.init_noise = {'shape': dist(tensor([.4, .4, .2])),\n",
        "                       'scale': dist(tensor([1., 2., 2., 2., 2., 1.])),\n",
        "                       'orien': dist(tensor(1/40).repeat(40)),\n",
        "                       'posY': dist(tensor(1/32).repeat(32))}\n",
        "    \n",
        "    def f_cat(N):\n",
        "      return N\n",
        "\n",
        "    def model(noise  = self.init_noise):\n",
        "      #noise variables\n",
        "      N_shape = pyro.sample('N_shape', noise['shape'])\n",
        "      N_scale = pyro.sample('N_scale', noise['scale'])\n",
        "      N_orien = pyro.sample('N_orien', noise['orien'])\n",
        "      N_posY = pyro.sample('N_posY', noise['posY'])\n",
        "      #variables\n",
        "      shape = pyro.sample('shape', Normal(f_cat(N_shape), tensor(0.01)).to_event(1))\n",
        "      scale = pyro.sample('scale', Normal(f_cat(N_scale), tensor(0.01)).to_event(1))\n",
        "      orien = pyro.sample('orien', Normal(f_cat(N_orien), tensor(0.01)).to_event(1))\n",
        "      posY = pyro.sample('posY', Normal(f_cat(N_posY), tensor(0.01)).to_event(1))\n",
        "      return shape, scale, orien, posY\n",
        "\n",
        "    def guide(noise = self.init_noise):\n",
        "      #noise params\n",
        "      prob_shape = pyro.param('prob_shape', tensor(1/3).repeat(3), constraint = constraints.positive)\n",
        "      prob_scale = pyro.param('prob_scale', tensor(1/6).repeat(6), constraint = constraints.positive)\n",
        "      prob_orien = pyro.param('prob_orien', tensor(1/40).repeat(40), constraint = constraints.positive)\n",
        "      prob_posY = pyro.param('prob_posY', tensor(1/32).repeat(32), constraint = constraints.positive)\n",
        "      #noise variables\n",
        "      N_shape = pyro.sample('N_shape', OneHotCategorical(prob_shape))\n",
        "      N_scale = pyro.sample('N_scale', OneHotCategorical(prob_scale))\n",
        "      N_orien = pyro.sample('N_orien', OneHotCategorical(prob_orien))\n",
        "      N_posY= pyro.sample('N_posY', OneHotCategorical(prob_posY))\n",
        "      #variables\n",
        "      shape = pyro.sample('shape', Normal(f_cat(N_shape), tensor(0.01)).to_event(1), infer={'is_auxiliary': True})\n",
        "      scale = pyro.sample('scale', Normal(f_cat(N_scale), tensor(0.01)).to_event(1), infer={'is_auxiliary': True})\n",
        "      orien = pyro.sample('orien', Normal(f_cat(N_orien), tensor(0.01)).to_event(1), infer={'is_auxiliary': True})\n",
        "      posY = pyro.sample('posY', Normal(f_cat(N_posY), tensor(0.01)).to_event(1), infer={'is_auxiliary': True})\n",
        "      return\n",
        "\n",
        "    self.model = model\n",
        "    self.guide = guide\n",
        "scm = SCM_test(vae)\n",
        "scm.model()\n",
        "\n",
        "data = {'shape': tensor([0., 1., 0.]),\n",
        "        'scale': tensor([1., 0., 0., 0., 0., 0.]),\n",
        "        'orien': torch.nn.functional.one_hot(tensor(5), 40).to(torch.float32),\n",
        "        'posY': torch.nn.functional.one_hot(tensor(28), 32).to(torch.float32)}\n",
        "\n",
        "condModel = pyro.condition(scm.model, data = data)\n",
        "pyro.clear_param_store()\n",
        "\n",
        "#setting the SVI attributes\n",
        "adam_params = {'lr': 0.1}\n",
        "optmizer = pyro.optim.Adam(adam_params)\n",
        "elbo = pyro.infer.Trace_ELBO()\n",
        "svi = pyro.infer.SVI(condModel, scm.guide, optmizer, elbo)\n",
        "var = ['shape', 'scale', 'orien', 'posY']\n",
        "#training loop\n",
        "cat_samples = {'shape':[], 'scale':[], 'orien':[], 'posY':[]}\n",
        "losses = []\n",
        "for i in range(1000):\n",
        "  losses.append(svi.step(scm.init_noise))\n",
        "  #saving samples\n",
        "  for key in var:\n",
        "    cat_samples[key].append(pyro.param('prob_'+key))\n",
        "for key in var:\n",
        "  cat_samples[key] = t_mean(cat_samples[key])\n",
        "\n",
        "updated_noise = {key : OneHotCategorical(cat_samples[key]) for key in var}\n",
        "\n",
        "shape_samples = []\n",
        "scale_samples = []\n",
        "orien_samples = []\n",
        "posy_samples = []\n",
        "for _ in range(1000):\n",
        "  a, b, c , d= scm.model(updated_noise)\n",
        "  shape_samples.append(a)\n",
        "  scale_samples.append(b)\n",
        "  orien_samples.append(c)\n",
        "  posy_samples.append(d)\n",
        "shape_samples = t_mean(shape_samples)\n",
        "scale_samples = t_mean(scale_samples)\n",
        "orien_samples = t_mean(orien_samples)\n",
        "posy_samples = t_mean(posy_samples)\n",
        "print(shape_samples[1], scale_samples[0], orien_samples[5], posy_samples[28])     \n"
      ],
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.9530) tensor(0.9071) tensor(0.9746) tensor(0.9879)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69XahpNRJIXZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c6ec4065-8f8e-43dc-ac0b-82961e1707cb"
      },
      "source": [
        "scm.model()"
      ],
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.0078,  0.9958, -0.0110])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 170
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9MdZ9wSLYyJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "outputId": "3317b715-a912-40b2-d6fc-98ebcbcd9e6f"
      },
      "source": [
        "plt.scatter(range(len(losses)), losses)\n",
        "plt.show()"
      ],
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD4CAYAAAAO9oqkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmZnhZGBmZjgZmJkZ8P8BfhJLiEljpS8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfljchB2Ty5c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}