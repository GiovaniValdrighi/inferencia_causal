{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vae.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GiovaniValdrighi/inferencia_causal/blob/master/vae.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd2vuD3C7Bd4",
        "colab_type": "text"
      },
      "source": [
        "Estudo de variational autoencoder através do texto: https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73\n",
        "\n",
        "Implementação de variational autoencoder com Pyro através do texto: https://pyro.ai/examples/vae.html\n",
        "\n",
        "#Variational Autoencoder\n",
        "\n",
        "De modo simples, um **autoencoder** é um método utilizado para diminuir a dimensão de informação. Utilizando de redes neurais criamos duas, um **encoder** e um **decoder**. Seja nossa informação **X** uma matriz de dimensão **(n m)**, o encoder é uma rede neural responsável por transformar **X** para uma dimensão menor, se tornará o que chamamos de **variável latente**, **Z**. Em seguida, o decoder é responsável por transformar **Z** para as dimensões originais, teremos a matriz **X'**. O aprendizado da rede neural ocorre atráves da otimização da distância entre **X** e **X'**. \n",
        "\n",
        "No entanto, em um autoencoder simples, o espaço latente não é estável, isto é, valores similhares de **Z** não são similares à **X'**. Para isto, utilizamos de um **variational autoencoder**. A diferença é que transformamos **X** em uma distribuição no espaço latente, e não um valor constante. Assim, para recuperarmos com o decoder, utilizamos de uma amostra aleatória da distribuição do espaço latente, e por consequência do método de treinamento, teremos um decoder que leva amostras aleatórias semlehantes para **X'** semelhantes. Por conveniência, a distribuição do espaço latente é uma Gaussiana, sendo assim,o encoder ao receber **X** retorna a média e a variância da distribuição, geramos uma amostra aleatória de uma normal padrão e por fim repassamos a variável latente para o decoder.\n",
        "\n",
        "O nosso objetivo é utilizar o banco de dados Deepmind Dsprites que contém um conjunto de figuras 64x64 com 6 características: cor, forma, escala, orientação, posição no eixo X e posição no eixo Y. Geralmente, este banco de dados é utilizado para a verificação de métodos de \"desembaraço\" (desintanglement), isto é, desejamos avaliar a capacidade das dimensões do espaço latente representarem as características que geraram o banco de dados. Nesta situação, iremos utilizar esse banco de dados para construir uma SCM e realizar perguntas contrafactuais. \n",
        "\n",
        "A primeira etapa é a construção de um variational autoencoder simples utilizando o banco de dados. Para lidar com a distribuição, iremos utilizar da biblioteca Pyro. Possuímos duas redes neurais, o encoder e o decoder.\n",
        "\n",
        "## Encoder\n",
        "Rede neural resposável por receber um vetor **img** 1x4096 dos pixels de uma figura e um vetor **label** categórico 1x114 contendo as características (cor, forma, escala, orientação, pos. X, pos. Y) da figura. Os dois vetores são concatenados em um vetor **data** 1x4210. Realizamos uma transformação linear que vai do espaço 1x4210 para o espaço 1x1000, criando a variável **hidden**, em seguida, transformamos do espaço 1x1000 para o espaço latente de dimensão 1x200. Geramos duas variáveis latentes utilizando de duas transformações lineares: a **média** (vetor 1x200) e a **variância** (vetor 1x200). \n",
        "\n",
        "\n",
        "dúvidas:\n",
        "- eu que escolho quantas transformaçoes lineares usar? e a dimensão do espaço latente, também posso escolher?\n",
        "- por que usar a softplus? li que é uma função de ativação, não sei exatamente o que é\n",
        "\n",
        "##Decoder\n",
        "Rede neural responsável por receber um vetor **latente** 1x200 e um vetor **label** categórico 1x114. Os dois valores são concatenados em um **data** 1x314. Em seguida utilizamos uma transformação linear para o espaço 1x1000 gerando a variável **hidden**. A partir dessa variável, iremos realizar uma transformação linear para o espaço original 1x4210 obtendo a variável **img'**.\n",
        "\n",
        "dúvidas:\n",
        "- eu que escolho quantas transformaçoes lineares usar? e a dimensão do espaço latente, também posso escolher?\n",
        "- não sei exatamente porque se usa a sigmoid, mas também é uma função de ativação\n",
        "\n",
        "\n",
        "##Inferência variancional\n",
        "O método de inferência que iremos utilizar para as redes neurais utiliza do SVI do Pyro. Para o SVI devemos definir 2 funções estocásticas, o modelo e o guide. O modelo é uma função estocástica de variáveis latentes **z** retornando a imagem **x**. O guide é uma função estocástica da imagem **x** para as variáveis latents **z**.\n",
        "\n",
        "- eu não entendi 100% o método de treinamento do svi\n",
        "\n",
        "##Modelo\n",
        "O modelo é uma função estocástica que recebe a **imagem** 256x4096 e o **label** 256x114, 256 é a dimensão do nosso batch, utilizando o pyro.plate declaramos independência na dimensão do batch. Criamos um vetor média 256x200 e um vetor de variância 256x200 da normal padrão, em seguida geramos uma amostra da variável aleatória **latent** (definida com o pyro), e geramos uma variável aleatória para o label (será explicado depois), e passamos as duas amostras para decoder, que retornará uma matriz 256x4210 sendo a nossa imagem. Por fim, definimos a variável aleatória **loc**, que segue uma distribuição bernoulli com parâmetro igual a **loc** e valor observado igual a **imagem**. \n",
        "\n",
        "##Guide\n",
        "O guide é uma função estocástica que recebe a **imagem** 256x4096 e o **label** 256x114, 256 é a dimensão do nosso batch, utilizando o pyro.plate declaramos independência na dimensão do batch. Geramos uma variável aleatória para o label e passamos para o encoder a imagem e variável aleatória label, obtemos a média e a variância da normal e geramos uma amostra da variável aleatória **latent**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3NFYw1nGK5v",
        "colab_type": "code",
        "outputId": "0bfe509f-e4aa-4f64-f7b2-64a858aae365",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        }
      },
      "source": [
        "!pip3 install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.0-{platform}-linux_x86_64.whl\n",
        "!pip3 install torchvision\n",
        "!pip3 install pyro-ppl"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: torch-0.4.0-{platform}-linux_x86_64.whl is not a valid wheel filename.\u001b[0m\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.4.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.12.0)\n",
            "Requirement already satisfied: torch==1.3.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.17.4)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (4.3.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision) (0.46)\n",
            "Collecting pyro-ppl\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/c8/3a52883ab27c5503385a983e4451a1e2944c2baab66c0f7ffc1f84bca329/pyro_ppl-1.1.0-py3-none-any.whl (429kB)\n",
            "\u001b[K     |████████████████████████████████| 430kB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (3.1.0)\n",
            "Collecting tqdm>=4.36\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7f/32/5144caf0478b1f26bd9d97f510a47336cf4ac0f96c6bc3b5af20d4173920/tqdm-4.40.2-py2.py3-none-any.whl (55kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 10.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (1.17.4)\n",
            "Collecting pyro-api>=0.1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/c2/bc/6cdbd1929e32fff62a33592633c2cc0393c7f7739131ccc9c9c4e28ac8dd/pyro_api-0.1.1-py3-none-any.whl\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (1.3.1)\n",
            "Installing collected packages: tqdm, pyro-api, pyro-ppl\n",
            "  Found existing installation: tqdm 4.28.1\n",
            "    Uninstalling tqdm-4.28.1:\n",
            "      Successfully uninstalled tqdm-4.28.1\n",
            "Successfully installed pyro-api-0.1.1 pyro-ppl-1.1.0 tqdm-4.40.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tqdm"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BK8JOHlXF0kF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pyro\n",
        "import pyro.distributions\n",
        "import pyro.infer\n",
        "import pyro.optim\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import networkx as nx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNCIwjneGBfQ",
        "colab_type": "code",
        "outputId": "605fe50c-0a04-4fe8-e560-881731c25858",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-169e15bc-58ae-4bbb-9c52-c34e0329c6c3\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-169e15bc-58ae-4bbb-9c52-c34e0329c6c3\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving dsprites_ndarray_co1sh3sc6or40x32y32_64x64.npz to dsprites_ndarray_co1sh3sc6or40x32y32_64x64.npz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icsHBreVG7NK",
        "colab_type": "text"
      },
      "source": [
        "As propriedades latentes das imagens são: shape, scale, size, position X e position Y. Vou criar um DAG para inserir relações causais entre as variáveis e em seguida criar um SCM para este DAG."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRuT7fPdHN9A",
        "colab_type": "code",
        "outputId": "bcca1e1a-e191-4374-d85b-3fcc8d5285d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "dataset_zip = np.load('dsprites_ndarray_co1sh3sc6or40x32y32_64x64.npz', allow_pickle = True, encoding = 'bytes')\n",
        "print('Keys in the dataset:', dataset_zip.files)\n",
        "imgs = dataset_zip['imgs']\n",
        "latents_values = dataset_zip['latents_values']\n",
        "latents_classes = dataset_zip['latents_classes']\n",
        "latents_sizes = dataset_zip['metadata'][()][b'latents_sizes']\n",
        "latents_names = dataset_zip['metadata'][()][b'latents_names']"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Keys in the dataset: ['metadata', 'imgs', 'latents_classes', 'latents_values']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CBn8GcJDtk8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#function that pick images from the dataset and return the batchs\n",
        "#with test and training data\n",
        "def setup_data_loader(batch_size = 128, size_test = 1/5):\n",
        "  #create the dataframes for training and test from the dataset_zip['imgs]\n",
        "  #test with 1/5 of the data\n",
        "  index = np.random.permutation(imgs.shape[0])\n",
        "  train_df = torch.utils.data.TensorDataset(torch.from_numpy(imgs[index[int(size_test*imgs.shape[0]):]].astype(np.float32).reshape(-1, 4096)), \n",
        "                                            torch.from_numpy(latents_classes[index[int(size_test*imgs.shape[0]):]].astype(np.float32)))\n",
        "  test_df = torch.utils.data.TensorDataset(torch.from_numpy(imgs[:index[int(size_test*imgs.shape[0])]].astype(np.float32).reshape(-1, 4096)), \n",
        "                                           torch.from_numpy(latents_classes[:index[int(size_test*imgs.shape[0])]].astype(np.float32)))\n",
        "  #creates a iterable dataset to train and test, each iteration have batch_size rows of data\n",
        "  train_loader = torch.utils.data.DataLoader(train_df, batch_size, shuffle = False)\n",
        "  test_loader = torch.utils.data.DataLoader(test_df, batch_size, shuffle = False)\n",
        "  return train_loader, test_loader\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "  '''This class receive the images data as vectors 1x4096\n",
        "  and the labels of the figure in the image as a 1x114 vector (dummy variables) \n",
        "  and should encode it to the latent space as mean and variance of\n",
        "  a normal distribution\n",
        "\n",
        "  :param img_dim: dimension of image vector\n",
        "  :param label_dim: dimension of label vector\n",
        "  :param latent_dim: dimension of latent space, output\n",
        "  '''\n",
        "  def __init__(self, img_dim = 4096, label_dim = 114, latent_dim = 200):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.img_dim = img_dim\n",
        "    self.label_dim = label_dim\n",
        "    self.latent_dim = latent_dim \n",
        "    #linear transformations used\n",
        "    self.fc1 = nn.Linear(img_dim + label_dim, 1000)\n",
        "    self.fc21 = nn.Linear(1000, latent_dim)\n",
        "    self.fc22 = nn.Linear(1000, latent_dim)\n",
        "    #non-linear transformation used\n",
        "    self.softplus = nn.Softplus()\n",
        "\n",
        "  def forward(self, img, label):\n",
        "    #use the transformation to get the hidden variable\n",
        "    data = torch.cat((img, torch.tensor(label, dtype = torch.float32)), -1)\n",
        "    hidden = self.softplus(self.fc1(data))\n",
        "    #use the transformation to get the mean and the variance\n",
        "    mean_z = self.fc21(hidden)\n",
        "    cov_z = torch.exp(self.fc22(hidden))\n",
        "    return mean_z, cov_z\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  '''This class receive a sample of the latent variable\n",
        "  and return the image as a data vector 1x4096 and\n",
        "  the latent classes as a vector 1x5\n",
        "\n",
        "  :param img_dim: dimension of image vector\n",
        "  :param label_dim: dimension of label vector\n",
        "  :param latent_dim: dimension of latent space, output\n",
        "  '''\n",
        "  def __init__(self, img_dim = 4096, label_dim = 114, latent_dim = 200):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.img_dim = img_dim\n",
        "    self.label_dim = label_dim\n",
        "    self.latent_dim = latent_dim\n",
        "    #linear transformations used\n",
        "    self.fc1 = nn.Linear(latent_dim+label_dim, 1000)\n",
        "    self.fc2 = nn.Linear(1000, img_dim)\n",
        "    #non-linear transformations used\n",
        "    self.softplus = nn.Softplus()\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, latent, label):\n",
        "    #use the transformation to get the hidden variable\n",
        "    data = torch.cat((latent, torch.tensor(label, dtype = torch.float32)), -1)\n",
        "    hidden = self.softplus(self.fc1(data))\n",
        "    #use the transformation to get the image\n",
        "    image = self.sigmoid(self.fc2(hidden))\n",
        "    return image\n",
        "\n",
        "class VAE(nn.Module):\n",
        "  '''\n",
        "  This class define the p(z|x) and the p(x|z)\n",
        "  and use the scm model to call the encoder and\n",
        "  decoder\n",
        "\n",
        "  \n",
        "  :param img_dim: dimension of image vector\n",
        "  :param label_dim: dimension of label vector\n",
        "  :param latent_dim: dimension of latent space, output\n",
        "  '''\n",
        "  def __init__(self, latents_sizes, latents_names, img_dim = 4096, label_dim = 114, latent_dim = 200):\n",
        "    super(VAE, self).__init__()\n",
        "    #creating networks\n",
        "    self.encoder = Encoder(img_dim, label_dim, latent_dim)\n",
        "    self.decoder = Decoder(img_dim, label_dim, latent_dim)\n",
        "    self.img_dim = img_dim\n",
        "    self.label_dim = label_dim\n",
        "    self.latent_dim = latent_dim\n",
        "    self.latents_sizes = latents_sizes\n",
        "    self.latents_names = latents_names\n",
        "  \n",
        "  def label_variable(self, label):\n",
        "    new_label = []\n",
        "    for i, length in enumerate(self.latents_sizes):\n",
        "      prior = label.new_ones(torch.Size((label.shape[0], length))) / (1.0 *length)\n",
        "      new_label.append(pyro.sample(\"label_\" + str(self.latents_names[i]), \n",
        "                      pyro.distributions.OneHotCategorical(prior), \n",
        "                      obs = torch.nn.functional.one_hot(torch.tensor(label[:, i], dtype = torch.int64), int(length))))\n",
        "    new_label = torch.cat(new_label, -1)\n",
        "    return torch.tensor(new_label)\n",
        "\n",
        "  def model(self, img, label):\n",
        "    '''\n",
        "    Function in the VAE that defines\n",
        "    p(x|z)\n",
        "    '''\n",
        "    pyro.module(\"decoder\", self.decoder)\n",
        "    with pyro.plate(\"data\", img.shape[0]):\n",
        "      z_mean = img.new_zeros(torch.Size((img.shape[0], self.latent_dim)))\n",
        "      z_variance = img.new_ones(torch.Size((img.shape[0], self.latent_dim)))\n",
        "      z_sample = pyro.sample(\"latent\", pyro.distributions.Normal(z_mean, z_variance).to_event(1))\n",
        "      label = self.label_variable(label)\n",
        "      image = self.decoder.forward(z_sample, label)\n",
        "      pyro.sample(\"obs\", pyro.distributions.Bernoulli(image).to_event(1), obs = img)\n",
        "\n",
        "\n",
        "  def guide(self, img, label):\n",
        "    '''\n",
        "    Function that is the guide to the model\n",
        "    shape, scale, orientation, posX, posY = g(img)\n",
        "    the p(z|x) use on the encoder\n",
        "    '''\n",
        "    pyro.module(\"encoder\", self.encoder)\n",
        "    with pyro.plate(\"data\", img.shape[0]):\n",
        "      label = self.label_variable(label)\n",
        "      z_mean, z_variance = self.encoder.forward(img, label)\n",
        "      pyro.sample(\"latent\", pyro.distributions.Normal(z_mean, z_variance).to_event(1))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vu8kGO5K-Wyq",
        "colab_type": "code",
        "outputId": "a15d6a24-e9b1-4672-e2a7-fb5dd43aeb8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        }
      },
      "source": [
        "pyro.enable_validation(True)\n",
        "#the training routine\n",
        "train_loader, test_loader = setup_data_loader()\n",
        "vae = VAE(latents_sizes, latents_names)\n",
        "\n",
        "#optimizer\n",
        "optimizer = pyro.optim.Adam({\"lr\" : 1.0e-3})\n",
        "\n",
        "#inference algorithm\n",
        "elbo = pyro.infer.Trace_ELBO()\n",
        "svi = pyro.infer.SVI(vae.model, vae.guide, optimizer, elbo)\n",
        "\n",
        "train_elbo = []\n",
        "test_elbo = []\n",
        "num_epochs = 5\n",
        "test_freq = 1\n",
        "for epoch in range(num_epochs):\n",
        "  epoch_loss = 0\n",
        "  for (img, label) in train_loader:\n",
        "    epoch_loss += svi.step(img, label)\n",
        "  total_epoch_loss_train = epoch_loss/len(train_loader)\n",
        "  train_elbo.append(total_epoch_loss_train)\n",
        "  print(\"epoch: \" + str(epoch) + \" average training loss: \" + str(epoch_loss))\n",
        "\n",
        "  if epoch % test_freq == 0:\n",
        "    test_loss = 0\n",
        "    for (img, label) in test_loader:\n",
        "      test_loss += svi.evaluate_loss(img, label)\n",
        "    total_epoch_loss_test  = epoch_loss/len(test_loader)\n",
        "    test_elbo.append(total_epoch_loss_test)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:105: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.6/dist-packages/pyro/util.py:205: UserWarning: Found non-auxiliary vars in guide but not model, consider marking these infer={'is_auxiliary': True}:\n",
            "{\"label_b'orientation'\", \"label_b'posY'\", \"label_b'color'\", \"label_b'posX'\", \"label_b'shape'\", \"label_b'scale'\"}\n",
            "  guide_vars - aux_vars - model_vars))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 0 average training loss: 72931400.64794922\n",
            "epoch: 1 average training loss: 45281950.47314453\n",
            "epoch: 2 average training loss: 39851974.02783203\n",
            "epoch: 3 average training loss: 36726394.107910156\n",
            "epoch: 4 average training loss: 34487705.431640625\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tssNAb3Nvxmx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}