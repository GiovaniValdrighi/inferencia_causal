{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vae.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GiovaniValdrighi/inferencia_causal/blob/master/vae.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3NFYw1nGK5v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip3 install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.0-{platform}-linux_x86_64.whl\n",
        "!pip3 install torchvision\n",
        "!pip3 install pyro-ppl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BK8JOHlXF0kF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pyro\n",
        "import pyro.distributions\n",
        "import pyro.infer\n",
        "import pyro.optim\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import networkx as nx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd2vuD3C7Bd4",
        "colab_type": "text"
      },
      "source": [
        "Estudo de variational autoencoder através do texto: https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73\n",
        "\n",
        "Implementação de variational autoencoder com Pyro através do texto: https://pyro.ai/examples/vae.html\n",
        "\n",
        "#Variational Autoencoder\n",
        "\n",
        "De modo simples, um **autoencoder** é um método utilizado para diminuir a dimensão de informação. Utilizando de redes neurais criamos duas, um **encoder** e um **decoder**. Seja nossa informação **X** uma matriz de dimensão **(n m)**, o encoder é uma rede neural responsável por transformar **X** para uma dimensão menor, se tornará o que chamamos de **variável latente**, **Z**. Em seguida, o decoder é responsável por transformar **Z** para as dimensões originais, teremos a matriz **X'**. O aprendizado da rede neural ocorre atráves da otimização da distância entre **X** e **X'**. \n",
        "\n",
        "No entanto, em um autoencoder simples, o espaço latente não é estável, isto é, valores similhares de **Z** não são similares à **X'**. Para isto, utilizamos de um **variational autoencoder**. A diferença é que transformamos **X** em uma distribuição no espaço latente, e não um valor constante. Assim, para recuperarmos com o decoder, utilizamos de uma amostra aleatória da distribuição do espaço latente, e por consequência do método de treinamento, teremos um decoder que leva amostras aleatórias semlehantes para **X'** semelhantes.\n",
        "\n",
        "Por conveniência, a distribuição do espaço latente é uma Gaussiana, sendo assim,o encoder ao receber **X** retorna a média e a variância da distribuição, geramos uma amostra aleatória de uma normal padrão e por fim repassamos a variável latente para o decoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNCIwjneGBfQ",
        "colab_type": "code",
        "outputId": "e45dec62-7d1c-4a3f-9de8-2cb640203915",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a52e05c3-e938-4718-91e7-2cea28a421d5\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-a52e05c3-e938-4718-91e7-2cea28a421d5\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving dsprites_ndarray_co1sh3sc6or40x32y32_64x64.npz to dsprites_ndarray_co1sh3sc6or40x32y32_64x64 (1).npz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icsHBreVG7NK",
        "colab_type": "text"
      },
      "source": [
        "As propriedades latentes das imagens são: shape, scale, size, position X e position Y. Vou criar um DAG para inserir relações causais entre as variáveis e em seguida criar um SCM para este DAG."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRuT7fPdHN9A",
        "colab_type": "code",
        "outputId": "858e6ca5-0128-44e7-d7c7-0af4ffa823a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "dataset_zip = np.load('dsprites_ndarray_co1sh3sc6or40x32y32_64x64.npz', allow_pickle = True, encoding = 'bytes')\n",
        "print('Keys in the dataset:', dataset_zip.files)\n",
        "imgs = dataset_zip['imgs']\n",
        "latents_values = dataset_zip['latents_values']\n",
        "latents_classes = dataset_zip['latents_classes']\n",
        "latents_sizes = dataset_zip['metadata'][()][b'latents_sizes']\n",
        "latents_names = dataset_zip['metadata'][()][b'latents_names']"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Keys in the dataset: ['metadata', 'imgs', 'latents_classes', 'latents_values']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CBn8GcJDtk8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#function that pick images from the dataset and return the batchs\n",
        "#with test and training data\n",
        "def setup_data_loader(batch_size = 128, size_test = 1/5):\n",
        "  #create the dataframes for training and test from the dataset_zip['imgs]\n",
        "  #test with 1/5 of the data\n",
        "  index = np.random.permutation(imgs.shape[0])\n",
        "  train_df = torch.utils.data.TensorDataset(torch.from_numpy(imgs[index[int(size_test*imgs.shape[0]):]].astype(np.float32).reshape(-1, 4096)), \n",
        "                                            torch.from_numpy(latents_classes[index[int(size_test*imgs.shape[0]):]].astype(np.float32)))\n",
        "  test_df = torch.utils.data.TensorDataset(torch.from_numpy(imgs[:index[int(size_test*imgs.shape[0])]].astype(np.float32).reshape(-1, 4096)), \n",
        "                                           torch.from_numpy(latents_classes[:index[int(size_test*imgs.shape[0])]].astype(np.float32)))\n",
        "  #creates a iterable dataset to train and test, each iteration have batch_size rows of data\n",
        "  train_loader = torch.utils.data.DataLoader(train_df, batch_size, shuffle = False)\n",
        "  test_loader = torch.utils.data.DataLoader(test_df, batch_size, shuffle = False)\n",
        "  return train_loader, test_loader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpAOLUrWGGW2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "  '''This class receive the images data as vectors 1x4096\n",
        "  and the labels of the figure in the image as a 1x116 vector (dummy variables) \n",
        "  and should encode it to the latent space as mean and variance of\n",
        "  a normal distribution\n",
        "\n",
        "  :param img_dim: dimension of image vector\n",
        "  :param label_dim: dimension of label vector\n",
        "  :param latent_dim: dimension of latent space, output\n",
        "  '''\n",
        "  def __init__(self, img_dim = 4096, label_dim = 116, latent_dim = 200):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.img_dim = img_dim\n",
        "    self.label_dim = label_dim\n",
        "    self.latent_dim = latent_dim \n",
        "    #linear transformations used\n",
        "    self.fc1 = nn.Linear(img_dim + label_dim, 1000)\n",
        "    self.fc21 = nn.Linear(1000, latent_dim)\n",
        "    self.fc22 = nn.Linear(1000, latent_dim)\n",
        "    #non-linear transformation used\n",
        "    self.softplus = nn.Softplus()\n",
        "\n",
        "  def forward(self, img, label):\n",
        "    #use the transformation to get the hidden variable\n",
        "    data = torch.cat((img, label), -1)\n",
        "    hidden = self.softplus(self.fc1(data))\n",
        "    #use the transformation to get the mean and the variance\n",
        "    mean_z = self.fc21(hidden)\n",
        "    cov_z = torch.exp(self.fc22(hidden))\n",
        "    return mean_z, cov_z\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  '''This class receive a sample of the latent variable\n",
        "  and return the image as a data vector 1x4096 and\n",
        "  the latent classes as a vector 1x5\n",
        "\n",
        "  :param img_dim: dimension of image vector\n",
        "  :param label_dim: dimension of label vector\n",
        "  :param latent_dim: dimension of latent space, output\n",
        "  '''\n",
        "  def __init__(self, img_dim = 4096, label_dim = 116, latent_dim = 200):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.img_dim = img_dim\n",
        "    self.label_dim = label_dim\n",
        "    self.latent_dim = latent_dim\n",
        "    #linear transformations used\n",
        "    self.fc1 = nn.Linear(latent_dim+label_dim, 1000)\n",
        "    self.fc2 = nn.Linear(1000, img_dim)\n",
        "    #non-linear transformations used\n",
        "    self.softplus = nn.Softplus()\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, latent, label):\n",
        "    #use the transformation to get the hidden variable\n",
        "    data = torch.cat((latent, label), -1)\n",
        "    hidden = self.softplus(self.fc1(data))\n",
        "    #use the transformation to get the image\n",
        "    image = self.sigmoid(self.fc2(hidden))\n",
        "    return image\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujDjtt4zMjHm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VAE(nn.Module):\n",
        "  '''\n",
        "  This class define the p(z|x) and the p(x|z)\n",
        "  and use the scm model to call the encoder and\n",
        "  decoder\n",
        "\n",
        "  \n",
        "  :param img_dim: dimension of image vector\n",
        "  :param label_dim: dimension of label vector\n",
        "  :param latent_dim: dimension of latent space, output\n",
        "  '''\n",
        "  def __init__(self, latents_sizes, latents_names, img_dim = 4096, label_dim = 116, latent_dim = 200):\n",
        "    super(VAE, self).__init__()\n",
        "    #creating networks\n",
        "    self.encoder = Encoder(img_dim, label_dim, latent_dim)\n",
        "    self.decoder = Decoder(img_dim, label_dim, latent_dim)\n",
        "    self.img_dim = img_dim\n",
        "    self.label_dim = label_dim\n",
        "    self.latent_dim = latent_dim\n",
        "    self.latents_sizes = latents_sizes\n",
        "    self.latents_names = latents_names\n",
        "  \n",
        "  def label_variable(self, label):\n",
        "    new_label = []\n",
        "    for i, length in enumerate(self.latents_sizes):\n",
        "      prior = label.new_ones(torch.Size((label.shape[0], length))) / (1.0 *length)\n",
        "      new_label.append(pyro.sample(\"label_\" + self.latents_names[i], \n",
        "                      pyro.distributions.OneHotCategorical(prior), \n",
        "                      obs = torch.nn.functional.one_hot(torch.tensor(label[:, i], dtype = torch.int64), int(length))))\n",
        "    new_label = torch.cat(new_label, -1)\n",
        "    return torch.tensor(new_label)\n",
        "\n",
        "  def model(self, img, label):\n",
        "    '''\n",
        "    Function in the VAE that defines\n",
        "    p(x|z)\n",
        "    '''\n",
        "    pyro.module(\"decoder\", self.decoder)\n",
        "    with pyro.plate(\"data\", img.shape[0]):\n",
        "      z_mean = data.new_zeros(torch.Size((data.shape[0], self.latent_dim)))\n",
        "      z_variance = data.new_ones(torch.Size((data.shape[0], self.latent_dim)))\n",
        "      z_sample = pyro.sample(\"latent\", pyro.distributions.Normal(z_mean, z_variance).to_event(1))\n",
        "      label = self.label_variable(label)\n",
        "      image = self.decoder.forward(z_sample, label)\n",
        "      pyro.sample(\"obs\", pyro.distributions.Bernoulli(image).to_event(1), obs = img)\n",
        "\n",
        "\n",
        "  def guide(self, img, label):\n",
        "    '''\n",
        "    Function that is the guide to the model\n",
        "    shape, scale, orientation, posX, posY = g(img)\n",
        "    the p(z|x) use on the encoder\n",
        "    '''\n",
        "    pyro.module(\"encoder\", self.encoder)\n",
        "    with pyro.plate(\"data\", img.shape[0]):\n",
        "      label = self.label_variable(label)\n",
        "      z_mean, z_variance = self.encoder.forward(img, label)\n",
        "      pyro.sample(\"latent\", pyro.distributions.Normal(z_mean, z_variance).to_event(1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vu8kGO5K-Wyq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#the training routine\n",
        "train_loader, test_loader = setup_data_loader()\n",
        "vae = VAE(latents_sizes, latents_names)\n",
        "\n",
        "#optimizer\n",
        "optimizer = pyro.optim.Adam({\"lr\" : 1.0e-3})\n",
        "\n",
        "#inference algorithm\n",
        "elbo = pyro.infer.Trace_ELBO()\n",
        "svi = pyro.infer.SVI(vae.model, vae.guide, optimizer, elbo)\n",
        "\n",
        "train_elbo = []\n",
        "test_elbo = []\n",
        "num_epochs = 5\n",
        "test_freq = 1\n",
        "for epoch in range(num_epochs):\n",
        "  epoch_loss = 0\n",
        "  for (img, label) in train_loader:\n",
        "    epoch_loss += svi.step(img, label)\n",
        "  total_epoch_loss_train = epoch_loss/len(train_loader)\n",
        "  train_elbo.append(total_epoch_loss_train)\n",
        "  print(\"epoch: \" + str(epoch) + \" average training loss: \" + str(epoch_loss))\n",
        "\n",
        "  if epoch % test_freq == 0:\n",
        "    test_loss = 0\n",
        "    for (img, label) in test_loader:\n",
        "      test_loss += svi.evaluate_loss(img, label)\n",
        "    total_epoch_loss_test  = epoch_loss/len(test_loader)\n",
        "    test_elbo.append(total_epoch_loss_test)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}